{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29dc571a-e16a-47fa-b456-c4d9367b87a5",
   "metadata": {},
   "source": [
    "# Unit 3, Exercise 1: Banknote Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f20142-5f08-4b33-a4e5-ec1315c5a8ff",
   "metadata": {},
   "source": [
    "In this exercise, we are applying logistic regression to a banknote authentication dataset to distinguish between genuine and forged bank notes.\n",
    "\n",
    "\n",
    "**The dataset consists of 1372 examples and 4 features for binary classification.** The features are \n",
    "\n",
    "1. variance of a wavelet-transformed image (continuous) \n",
    "2. skewness of a wavelet-transformed image (continuous) \n",
    "3. kurtosis of a wavelet-transformed image (continuous) \n",
    "4. entropy of the image (continuous) \n",
    "\n",
    "(You can fine more details about this dataset at [https://archive.ics.uci.edu/ml/datasets/banknote+authentication](https://archive.ics.uci.edu/ml/datasets/banknote+authentication).)\n",
    "\n",
    "\n",
    "In essence, these four features represent features that were manually extracted from image data. Note that you do not need the details of these features for this exercise. \n",
    "\n",
    "However, you are encouraged to explore the dataset further, e.g., by plotting the features, looking at the value ranges, and so forth. (We will skip these steps for brevity in this exercise)\n",
    "\n",
    "Most of the code should look familiar to you since it is based on the logistic regression code from Unit 3.6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5649ce4a-7db3-4716-9fd1-d2db9af5f834",
   "metadata": {},
   "source": [
    "## 1) Installing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea64205-ec41-42db-8167-cd547453354f",
   "metadata": {},
   "source": [
    "You likely already have all libraries installed and don't need to do anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac7e723b-08af-4274-8925-bda4ef60f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install numpy pandas matplotlib --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee966b7b-27cb-4484-b256-8d79f55dc48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d5d7bb5-c818-4d8c-b6ce-9c6d8fb4ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3822a1f2-6b48-4826-9bf4-adbe06c65a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.13.1\n",
      "IPython version      : 8.30.0\n",
      "\n",
      "numpy     : 2.2.0\n",
      "pandas    : 2.2.3\n",
      "matplotlib: 3.10.0\n",
      "torch     : 2.6.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,matplotlib,torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c6970-2b47-49a1-ba50-59bf738526ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2) Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6638725c-02ee-44db-b661-d882dd191185",
   "metadata": {},
   "source": [
    "We are using the familiar `read_csv` function from pandas to load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab70cfad-f8bb-4076-b22e-dffa4f8a48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb8965ae-5222-4541-a7c6-7a9aaa4d1033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2        3  4\n",
       "0  3.62160  8.6661 -2.8073 -0.44699  0\n",
       "1  4.54590  8.1674 -2.4586 -1.46210  0\n",
       "2  3.86600 -2.6383  1.9242  0.10645  0\n",
       "3  3.45660  9.5228 -4.0112 -3.59440  0\n",
       "4  0.32924 -4.4552  4.5718 -0.98880  0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_banknote_authentication.txt\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "860304f1-1b8c-4993-b547-20e2dcceb03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = df[[0, 1, 2, 3]].values\n",
    "y_labels = df[4].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d2ebb2-d83f-4729-85ed-9437e105b9b8",
   "metadata": {},
   "source": [
    "Number of examples and features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f342b22-0fde-436a-a121-00e9ce627512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1372, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e94b9-4847-4833-a7d1-afee3c18991a",
   "metadata": {},
   "source": [
    "It is usually a good idea to look at the label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e8247a8-101d-4195-84d3-12b6593c0099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([762, 610])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.bincount(y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e7578-c57d-4aae-99fc-77603e202185",
   "metadata": {},
   "source": [
    "## 3) Defining a DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705b52c8-1635-40c8-a6f3-8c4d0d91952e",
   "metadata": {},
   "source": [
    "The `DataLoader` code is the same code we used in Unit 3.6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa4ba92f-f294-4572-8aa2-d2fa50788a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "\n",
    "        self.features = torch.tensor(X, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]        \n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2096f23-539a-40e0-affa-db0ffcd0f371",
   "metadata": {},
   "source": [
    "We will be using 80% of the data for training, 20% of the data for validation. In a real-project, we would also have a separate dataset for the final test set (in this case, we do not have an explicit test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8bbd768-c15c-40f5-8500-83fad5bb1722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1097"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(X_features.shape[0]*0.80)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b09d3d5-f4f7-47df-8160-8d883c0c5e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = X_features.shape[0] - train_size\n",
    "val_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e21d88e-1408-4457-a7fd-3306a9fac5a6",
   "metadata": {},
   "source": [
    "Using `torch.utils.data.random_split`, we generate the training and validation sets along with the respective data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22a0e19a-de40-4309-b197-368a781a5633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "dataset = MyDataset(X_features, y_labels)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_set,\n",
    "    batch_size=10,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a0b2f5-66f5-45e5-9b0a-f4960fc40388",
   "metadata": {},
   "source": [
    "## 4) Implementing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee409f0-02e0-4591-abf1-5e2c6c41a187",
   "metadata": {},
   "source": [
    "Here, we are resusing the same model code we used in Unit 3.6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3da86d9a-7cd5-467c-bf65-3388fe272bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(num_features, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x)\n",
    "        probas = torch.sigmoid(logits)\n",
    "        return probas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8340676-a3da-49cf-aeae-c0a3329734c5",
   "metadata": {},
   "source": [
    "## 5) The training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1660ca15-18d5-4a55-94f1-e9f543bd8748",
   "metadata": {},
   "source": [
    "In this section, we are using the training loop from Unit 3.6. It's the exact same code except for some small modification: We added the line `if not batch_idx % 20` to only print the loss for every 20th batch (to reduce the number of output lines).\n",
    "\n",
    "<font color='red'>YOUR TASK is to find a good learning rate and epoch number so that you achieve a training and validation performance of at least 98%.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79c712f6-4e2a-43e9-8563-215f88beb4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050 | Batch 000/110 | Loss: 1.30\n",
      "Epoch: 001/050 | Batch 020/110 | Loss: 0.85\n",
      "Epoch: 001/050 | Batch 040/110 | Loss: 0.66\n",
      "Epoch: 001/050 | Batch 060/110 | Loss: 0.37\n",
      "Epoch: 001/050 | Batch 080/110 | Loss: 0.28\n",
      "Epoch: 001/050 | Batch 100/110 | Loss: 0.29\n",
      "Epoch: 002/050 | Batch 000/110 | Loss: 0.39\n",
      "Epoch: 002/050 | Batch 020/110 | Loss: 0.22\n",
      "Epoch: 002/050 | Batch 040/110 | Loss: 0.35\n",
      "Epoch: 002/050 | Batch 060/110 | Loss: 0.19\n",
      "Epoch: 002/050 | Batch 080/110 | Loss: 0.19\n",
      "Epoch: 002/050 | Batch 100/110 | Loss: 0.18\n",
      "Epoch: 003/050 | Batch 000/110 | Loss: 0.13\n",
      "Epoch: 003/050 | Batch 020/110 | Loss: 0.28\n",
      "Epoch: 003/050 | Batch 040/110 | Loss: 0.21\n",
      "Epoch: 003/050 | Batch 060/110 | Loss: 0.37\n",
      "Epoch: 003/050 | Batch 080/110 | Loss: 0.18\n",
      "Epoch: 003/050 | Batch 100/110 | Loss: 0.14\n",
      "Epoch: 004/050 | Batch 000/110 | Loss: 0.15\n",
      "Epoch: 004/050 | Batch 020/110 | Loss: 0.07\n",
      "Epoch: 004/050 | Batch 040/110 | Loss: 0.17\n",
      "Epoch: 004/050 | Batch 060/110 | Loss: 0.16\n",
      "Epoch: 004/050 | Batch 080/110 | Loss: 0.05\n",
      "Epoch: 004/050 | Batch 100/110 | Loss: 0.12\n",
      "Epoch: 005/050 | Batch 000/110 | Loss: 0.33\n",
      "Epoch: 005/050 | Batch 020/110 | Loss: 0.14\n",
      "Epoch: 005/050 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 005/050 | Batch 060/110 | Loss: 0.22\n",
      "Epoch: 005/050 | Batch 080/110 | Loss: 0.20\n",
      "Epoch: 005/050 | Batch 100/110 | Loss: 0.23\n",
      "Epoch: 006/050 | Batch 000/110 | Loss: 0.18\n",
      "Epoch: 006/050 | Batch 020/110 | Loss: 0.17\n",
      "Epoch: 006/050 | Batch 040/110 | Loss: 0.28\n",
      "Epoch: 006/050 | Batch 060/110 | Loss: 0.17\n",
      "Epoch: 006/050 | Batch 080/110 | Loss: 0.09\n",
      "Epoch: 006/050 | Batch 100/110 | Loss: 0.16\n",
      "Epoch: 007/050 | Batch 000/110 | Loss: 0.08\n",
      "Epoch: 007/050 | Batch 020/110 | Loss: 0.02\n",
      "Epoch: 007/050 | Batch 040/110 | Loss: 0.13\n",
      "Epoch: 007/050 | Batch 060/110 | Loss: 0.04\n",
      "Epoch: 007/050 | Batch 080/110 | Loss: 0.19\n",
      "Epoch: 007/050 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 008/050 | Batch 000/110 | Loss: 0.10\n",
      "Epoch: 008/050 | Batch 020/110 | Loss: 0.09\n",
      "Epoch: 008/050 | Batch 040/110 | Loss: 0.06\n",
      "Epoch: 008/050 | Batch 060/110 | Loss: 0.20\n",
      "Epoch: 008/050 | Batch 080/110 | Loss: 0.07\n",
      "Epoch: 008/050 | Batch 100/110 | Loss: 0.24\n",
      "Epoch: 009/050 | Batch 000/110 | Loss: 0.10\n",
      "Epoch: 009/050 | Batch 020/110 | Loss: 0.10\n",
      "Epoch: 009/050 | Batch 040/110 | Loss: 0.05\n",
      "Epoch: 009/050 | Batch 060/110 | Loss: 0.12\n",
      "Epoch: 009/050 | Batch 080/110 | Loss: 0.08\n",
      "Epoch: 009/050 | Batch 100/110 | Loss: 0.06\n",
      "Epoch: 010/050 | Batch 000/110 | Loss: 0.14\n",
      "Epoch: 010/050 | Batch 020/110 | Loss: 0.07\n",
      "Epoch: 010/050 | Batch 040/110 | Loss: 0.09\n",
      "Epoch: 010/050 | Batch 060/110 | Loss: 0.12\n",
      "Epoch: 010/050 | Batch 080/110 | Loss: 0.10\n",
      "Epoch: 010/050 | Batch 100/110 | Loss: 0.01\n",
      "Reached desired loss, stopping training.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 50\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = LogisticRegression(num_features=4)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) ## FILL IN VALUE\n",
    "\n",
    "num_epochs = num_epochs  ## FILL IN VALUE\n",
    "stop_training = False\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model = model.train()\n",
    "    for batch_idx, (features, class_labels) in enumerate(train_loader):\n",
    "\n",
    "        probas = model(features)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(probas, class_labels.view(probas.shape))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 20: # log every 20th batch\n",
    "            print(f'Epoch: {epoch+1:03d}/{num_epochs:03d}'\n",
    "                   f' | Batch {batch_idx:03d}/{len(train_loader):03d}'\n",
    "                   f' | Loss: {loss:.2f}')\n",
    "        if loss < 0.01:\n",
    "            print('Reached desired loss, stopping training.')\n",
    "            stop_training = True\n",
    "            break\n",
    "    if stop_training:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db1fc80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_logistic_model(model, train_loader, learning_rate=0.01, num_epochs=50):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : torch.nn.Module\n",
    "        The logistic regression model to train\n",
    "    train_loader : torch.utils.data.DataLoader\n",
    "        DataLoader containing the training data\n",
    "    learning_rate : float, default=0.01\n",
    "        Learning rate for the optimizer\n",
    "    num_epochs : int, default=50\n",
    "        Maximum number of epochs to train for\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        The epoch at which training stopped (either reached desired performance or completed all epochs)\n",
    "    \"\"\"\n",
    "    torch.manual_seed(1)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    stop_training = False\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model = model.train()\n",
    "        for batch_idx, (features, class_labels) in enumerate(train_loader):\n",
    "\n",
    "            probas = model(features)\n",
    "            \n",
    "            loss = F.binary_cross_entropy(probas, class_labels.view(probas.shape))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Logging\n",
    "            if not batch_idx % 20:  # log every 20th batch\n",
    "                print(f'Epoch: {epoch+1:03d}/{num_epochs:03d}'\n",
    "                      f' | Batch {batch_idx:03d}/{len(train_loader):03d}'\n",
    "                      f' | Loss: {loss:.2f}')\n",
    "            \n",
    "            if loss < 0.01:\n",
    "                print('Reached desired loss, stopping training.')\n",
    "                stop_training = True\n",
    "                break\n",
    "                \n",
    "        if stop_training:\n",
    "            break\n",
    "    \n",
    "    return epoch + 1  # Return the epoch number (1-indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0a5bff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001\n",
      "Epoch: 001/050 | Batch 000/110 | Loss: 0.95\n",
      "Epoch: 001/050 | Batch 020/110 | Loss: 1.04\n",
      "Epoch: 001/050 | Batch 040/110 | Loss: 1.06\n",
      "Epoch: 001/050 | Batch 060/110 | Loss: 0.81\n",
      "Epoch: 001/050 | Batch 080/110 | Loss: 0.99\n",
      "Epoch: 001/050 | Batch 100/110 | Loss: 0.63\n",
      "Epoch: 002/050 | Batch 000/110 | Loss: 0.55\n",
      "Epoch: 002/050 | Batch 020/110 | Loss: 0.87\n",
      "Epoch: 002/050 | Batch 040/110 | Loss: 0.68\n",
      "Epoch: 002/050 | Batch 060/110 | Loss: 0.47\n",
      "Epoch: 002/050 | Batch 080/110 | Loss: 0.59\n",
      "Epoch: 002/050 | Batch 100/110 | Loss: 0.53\n",
      "Epoch: 003/050 | Batch 000/110 | Loss: 0.69\n",
      "Epoch: 003/050 | Batch 020/110 | Loss: 0.42\n",
      "Epoch: 003/050 | Batch 040/110 | Loss: 0.51\n",
      "Epoch: 003/050 | Batch 060/110 | Loss: 0.28\n",
      "Epoch: 003/050 | Batch 080/110 | Loss: 0.56\n",
      "Epoch: 003/050 | Batch 100/110 | Loss: 0.53\n",
      "Epoch: 004/050 | Batch 000/110 | Loss: 0.42\n",
      "Epoch: 004/050 | Batch 020/110 | Loss: 0.53\n",
      "Epoch: 004/050 | Batch 040/110 | Loss: 0.38\n",
      "Epoch: 004/050 | Batch 060/110 | Loss: 0.46\n",
      "Epoch: 004/050 | Batch 080/110 | Loss: 0.34\n",
      "Epoch: 004/050 | Batch 100/110 | Loss: 0.69\n",
      "Epoch: 005/050 | Batch 000/110 | Loss: 0.61\n",
      "Epoch: 005/050 | Batch 020/110 | Loss: 0.54\n",
      "Epoch: 005/050 | Batch 040/110 | Loss: 0.46\n",
      "Epoch: 005/050 | Batch 060/110 | Loss: 0.22\n",
      "Epoch: 005/050 | Batch 080/110 | Loss: 0.32\n",
      "Epoch: 005/050 | Batch 100/110 | Loss: 0.27\n",
      "Epoch: 006/050 | Batch 000/110 | Loss: 0.40\n",
      "Epoch: 006/050 | Batch 020/110 | Loss: 0.30\n",
      "Epoch: 006/050 | Batch 040/110 | Loss: 0.48\n",
      "Epoch: 006/050 | Batch 060/110 | Loss: 0.45\n",
      "Epoch: 006/050 | Batch 080/110 | Loss: 0.32\n",
      "Epoch: 006/050 | Batch 100/110 | Loss: 0.46\n",
      "Epoch: 007/050 | Batch 000/110 | Loss: 0.27\n",
      "Epoch: 007/050 | Batch 020/110 | Loss: 0.30\n",
      "Epoch: 007/050 | Batch 040/110 | Loss: 0.33\n",
      "Epoch: 007/050 | Batch 060/110 | Loss: 0.27\n",
      "Epoch: 007/050 | Batch 080/110 | Loss: 0.27\n",
      "Epoch: 007/050 | Batch 100/110 | Loss: 0.33\n",
      "Epoch: 008/050 | Batch 000/110 | Loss: 0.34\n",
      "Epoch: 008/050 | Batch 020/110 | Loss: 0.14\n",
      "Epoch: 008/050 | Batch 040/110 | Loss: 0.18\n",
      "Epoch: 008/050 | Batch 060/110 | Loss: 0.23\n",
      "Epoch: 008/050 | Batch 080/110 | Loss: 0.26\n",
      "Epoch: 008/050 | Batch 100/110 | Loss: 0.32\n",
      "Epoch: 009/050 | Batch 000/110 | Loss: 0.25\n",
      "Epoch: 009/050 | Batch 020/110 | Loss: 0.32\n",
      "Epoch: 009/050 | Batch 040/110 | Loss: 0.30\n",
      "Epoch: 009/050 | Batch 060/110 | Loss: 0.29\n",
      "Epoch: 009/050 | Batch 080/110 | Loss: 0.16\n",
      "Epoch: 009/050 | Batch 100/110 | Loss: 0.19\n",
      "Epoch: 010/050 | Batch 000/110 | Loss: 0.31\n",
      "Epoch: 010/050 | Batch 020/110 | Loss: 0.34\n",
      "Epoch: 010/050 | Batch 040/110 | Loss: 0.49\n",
      "Epoch: 010/050 | Batch 060/110 | Loss: 0.31\n",
      "Epoch: 010/050 | Batch 080/110 | Loss: 0.39\n",
      "Epoch: 010/050 | Batch 100/110 | Loss: 0.27\n",
      "Epoch: 011/050 | Batch 000/110 | Loss: 0.17\n",
      "Epoch: 011/050 | Batch 020/110 | Loss: 0.21\n",
      "Epoch: 011/050 | Batch 040/110 | Loss: 0.23\n",
      "Epoch: 011/050 | Batch 060/110 | Loss: 0.17\n",
      "Epoch: 011/050 | Batch 080/110 | Loss: 0.29\n",
      "Epoch: 011/050 | Batch 100/110 | Loss: 0.18\n",
      "Epoch: 012/050 | Batch 000/110 | Loss: 0.33\n",
      "Epoch: 012/050 | Batch 020/110 | Loss: 0.38\n",
      "Epoch: 012/050 | Batch 040/110 | Loss: 0.15\n",
      "Epoch: 012/050 | Batch 060/110 | Loss: 0.29\n",
      "Epoch: 012/050 | Batch 080/110 | Loss: 0.35\n",
      "Epoch: 012/050 | Batch 100/110 | Loss: 0.19\n",
      "Epoch: 013/050 | Batch 000/110 | Loss: 0.15\n",
      "Epoch: 013/050 | Batch 020/110 | Loss: 0.23\n",
      "Epoch: 013/050 | Batch 040/110 | Loss: 0.46\n",
      "Epoch: 013/050 | Batch 060/110 | Loss: 0.27\n",
      "Epoch: 013/050 | Batch 080/110 | Loss: 0.37\n",
      "Epoch: 013/050 | Batch 100/110 | Loss: 0.37\n",
      "Epoch: 014/050 | Batch 000/110 | Loss: 0.40\n",
      "Epoch: 014/050 | Batch 020/110 | Loss: 0.28\n",
      "Epoch: 014/050 | Batch 040/110 | Loss: 0.19\n",
      "Epoch: 014/050 | Batch 060/110 | Loss: 0.26\n",
      "Epoch: 014/050 | Batch 080/110 | Loss: 0.18\n",
      "Epoch: 014/050 | Batch 100/110 | Loss: 0.17\n",
      "Epoch: 015/050 | Batch 000/110 | Loss: 0.32\n",
      "Epoch: 015/050 | Batch 020/110 | Loss: 0.25\n",
      "Epoch: 015/050 | Batch 040/110 | Loss: 0.26\n",
      "Epoch: 015/050 | Batch 060/110 | Loss: 0.27\n",
      "Epoch: 015/050 | Batch 080/110 | Loss: 0.20\n",
      "Epoch: 015/050 | Batch 100/110 | Loss: 0.13\n",
      "Epoch: 016/050 | Batch 000/110 | Loss: 0.25\n",
      "Epoch: 016/050 | Batch 020/110 | Loss: 0.09\n",
      "Epoch: 016/050 | Batch 040/110 | Loss: 0.30\n",
      "Epoch: 016/050 | Batch 060/110 | Loss: 0.24\n",
      "Epoch: 016/050 | Batch 080/110 | Loss: 0.31\n",
      "Epoch: 016/050 | Batch 100/110 | Loss: 0.11\n",
      "Epoch: 017/050 | Batch 000/110 | Loss: 0.24\n",
      "Epoch: 017/050 | Batch 020/110 | Loss: 0.15\n",
      "Epoch: 017/050 | Batch 040/110 | Loss: 0.19\n",
      "Epoch: 017/050 | Batch 060/110 | Loss: 0.24\n",
      "Epoch: 017/050 | Batch 080/110 | Loss: 0.29\n",
      "Epoch: 017/050 | Batch 100/110 | Loss: 0.12\n",
      "Epoch: 018/050 | Batch 000/110 | Loss: 0.29\n",
      "Epoch: 018/050 | Batch 020/110 | Loss: 0.27\n",
      "Epoch: 018/050 | Batch 040/110 | Loss: 0.30\n",
      "Epoch: 018/050 | Batch 060/110 | Loss: 0.22\n",
      "Epoch: 018/050 | Batch 080/110 | Loss: 0.33\n",
      "Epoch: 018/050 | Batch 100/110 | Loss: 0.38\n",
      "Epoch: 019/050 | Batch 000/110 | Loss: 0.21\n",
      "Epoch: 019/050 | Batch 020/110 | Loss: 0.09\n",
      "Epoch: 019/050 | Batch 040/110 | Loss: 0.28\n",
      "Epoch: 019/050 | Batch 060/110 | Loss: 0.27\n",
      "Epoch: 019/050 | Batch 080/110 | Loss: 0.24\n",
      "Epoch: 019/050 | Batch 100/110 | Loss: 0.16\n",
      "Epoch: 020/050 | Batch 000/110 | Loss: 0.12\n",
      "Epoch: 020/050 | Batch 020/110 | Loss: 0.21\n",
      "Epoch: 020/050 | Batch 040/110 | Loss: 0.36\n",
      "Epoch: 020/050 | Batch 060/110 | Loss: 0.35\n",
      "Epoch: 020/050 | Batch 080/110 | Loss: 0.18\n",
      "Epoch: 020/050 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 021/050 | Batch 000/110 | Loss: 0.28\n",
      "Epoch: 021/050 | Batch 020/110 | Loss: 0.18\n",
      "Epoch: 021/050 | Batch 040/110 | Loss: 0.16\n",
      "Epoch: 021/050 | Batch 060/110 | Loss: 0.32\n",
      "Epoch: 021/050 | Batch 080/110 | Loss: 0.32\n",
      "Epoch: 021/050 | Batch 100/110 | Loss: 0.18\n",
      "Epoch: 022/050 | Batch 000/110 | Loss: 0.10\n",
      "Epoch: 022/050 | Batch 020/110 | Loss: 0.35\n",
      "Epoch: 022/050 | Batch 040/110 | Loss: 0.25\n",
      "Epoch: 022/050 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 022/050 | Batch 080/110 | Loss: 0.10\n",
      "Epoch: 022/050 | Batch 100/110 | Loss: 0.08\n",
      "Epoch: 023/050 | Batch 000/110 | Loss: 0.14\n",
      "Epoch: 023/050 | Batch 020/110 | Loss: 0.25\n",
      "Epoch: 023/050 | Batch 040/110 | Loss: 0.26\n",
      "Epoch: 023/050 | Batch 060/110 | Loss: 0.24\n",
      "Epoch: 023/050 | Batch 080/110 | Loss: 0.14\n",
      "Epoch: 023/050 | Batch 100/110 | Loss: 0.19\n",
      "Epoch: 024/050 | Batch 000/110 | Loss: 0.28\n",
      "Epoch: 024/050 | Batch 020/110 | Loss: 0.19\n",
      "Epoch: 024/050 | Batch 040/110 | Loss: 0.41\n",
      "Epoch: 024/050 | Batch 060/110 | Loss: 0.18\n",
      "Epoch: 024/050 | Batch 080/110 | Loss: 0.10\n",
      "Epoch: 024/050 | Batch 100/110 | Loss: 0.35\n",
      "Epoch: 025/050 | Batch 000/110 | Loss: 0.09\n",
      "Epoch: 025/050 | Batch 020/110 | Loss: 0.11\n",
      "Epoch: 025/050 | Batch 040/110 | Loss: 0.06\n",
      "Epoch: 025/050 | Batch 060/110 | Loss: 0.05\n",
      "Epoch: 025/050 | Batch 080/110 | Loss: 0.18\n",
      "Epoch: 025/050 | Batch 100/110 | Loss: 0.15\n",
      "Epoch: 026/050 | Batch 000/110 | Loss: 0.14\n",
      "Epoch: 026/050 | Batch 020/110 | Loss: 0.12\n",
      "Epoch: 026/050 | Batch 040/110 | Loss: 0.08\n",
      "Epoch: 026/050 | Batch 060/110 | Loss: 0.26\n",
      "Epoch: 026/050 | Batch 080/110 | Loss: 0.25\n",
      "Epoch: 026/050 | Batch 100/110 | Loss: 0.18\n",
      "Epoch: 027/050 | Batch 000/110 | Loss: 0.11\n",
      "Epoch: 027/050 | Batch 020/110 | Loss: 0.16\n",
      "Epoch: 027/050 | Batch 040/110 | Loss: 0.15\n",
      "Epoch: 027/050 | Batch 060/110 | Loss: 0.14\n",
      "Epoch: 027/050 | Batch 080/110 | Loss: 0.16\n",
      "Epoch: 027/050 | Batch 100/110 | Loss: 0.09\n",
      "Epoch: 028/050 | Batch 000/110 | Loss: 0.16\n",
      "Epoch: 028/050 | Batch 020/110 | Loss: 0.16\n",
      "Epoch: 028/050 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 028/050 | Batch 060/110 | Loss: 0.18\n",
      "Epoch: 028/050 | Batch 080/110 | Loss: 0.17\n",
      "Epoch: 028/050 | Batch 100/110 | Loss: 0.14\n",
      "Epoch: 029/050 | Batch 000/110 | Loss: 0.21\n",
      "Epoch: 029/050 | Batch 020/110 | Loss: 0.16\n",
      "Epoch: 029/050 | Batch 040/110 | Loss: 0.33\n",
      "Epoch: 029/050 | Batch 060/110 | Loss: 0.15\n",
      "Epoch: 029/050 | Batch 080/110 | Loss: 0.19\n",
      "Epoch: 029/050 | Batch 100/110 | Loss: 0.06\n",
      "Epoch: 030/050 | Batch 000/110 | Loss: 0.17\n",
      "Epoch: 030/050 | Batch 020/110 | Loss: 0.12\n",
      "Epoch: 030/050 | Batch 040/110 | Loss: 0.09\n",
      "Epoch: 030/050 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 030/050 | Batch 080/110 | Loss: 0.06\n",
      "Epoch: 030/050 | Batch 100/110 | Loss: 0.24\n",
      "Epoch: 031/050 | Batch 000/110 | Loss: 0.05\n",
      "Epoch: 031/050 | Batch 020/110 | Loss: 0.16\n",
      "Epoch: 031/050 | Batch 040/110 | Loss: 0.16\n",
      "Epoch: 031/050 | Batch 060/110 | Loss: 0.18\n",
      "Epoch: 031/050 | Batch 080/110 | Loss: 0.18\n",
      "Epoch: 031/050 | Batch 100/110 | Loss: 0.35\n",
      "Epoch: 032/050 | Batch 000/110 | Loss: 0.07\n",
      "Epoch: 032/050 | Batch 020/110 | Loss: 0.14\n",
      "Epoch: 032/050 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 032/050 | Batch 060/110 | Loss: 0.22\n",
      "Epoch: 032/050 | Batch 080/110 | Loss: 0.19\n",
      "Epoch: 032/050 | Batch 100/110 | Loss: 0.16\n",
      "Epoch: 033/050 | Batch 000/110 | Loss: 0.26\n",
      "Epoch: 033/050 | Batch 020/110 | Loss: 0.15\n",
      "Epoch: 033/050 | Batch 040/110 | Loss: 0.17\n",
      "Epoch: 033/050 | Batch 060/110 | Loss: 0.11\n",
      "Epoch: 033/050 | Batch 080/110 | Loss: 0.19\n",
      "Epoch: 033/050 | Batch 100/110 | Loss: 0.18\n",
      "Epoch: 034/050 | Batch 000/110 | Loss: 0.37\n",
      "Epoch: 034/050 | Batch 020/110 | Loss: 0.29\n",
      "Epoch: 034/050 | Batch 040/110 | Loss: 0.09\n",
      "Epoch: 034/050 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 034/050 | Batch 080/110 | Loss: 0.11\n",
      "Epoch: 034/050 | Batch 100/110 | Loss: 0.16\n",
      "Epoch: 035/050 | Batch 000/110 | Loss: 0.26\n",
      "Epoch: 035/050 | Batch 020/110 | Loss: 0.19\n",
      "Epoch: 035/050 | Batch 040/110 | Loss: 0.23\n",
      "Epoch: 035/050 | Batch 060/110 | Loss: 0.03\n",
      "Epoch: 035/050 | Batch 080/110 | Loss: 0.05\n",
      "Epoch: 035/050 | Batch 100/110 | Loss: 0.19\n",
      "Epoch: 036/050 | Batch 000/110 | Loss: 0.22\n",
      "Epoch: 036/050 | Batch 020/110 | Loss: 0.11\n",
      "Epoch: 036/050 | Batch 040/110 | Loss: 0.08\n",
      "Epoch: 036/050 | Batch 060/110 | Loss: 0.18\n",
      "Epoch: 036/050 | Batch 080/110 | Loss: 0.15\n",
      "Epoch: 036/050 | Batch 100/110 | Loss: 0.14\n",
      "Epoch: 037/050 | Batch 000/110 | Loss: 0.14\n",
      "Epoch: 037/050 | Batch 020/110 | Loss: 0.12\n",
      "Epoch: 037/050 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 037/050 | Batch 060/110 | Loss: 0.26\n",
      "Epoch: 037/050 | Batch 080/110 | Loss: 0.20\n",
      "Epoch: 037/050 | Batch 100/110 | Loss: 0.16\n",
      "Epoch: 038/050 | Batch 000/110 | Loss: 0.13\n",
      "Epoch: 038/050 | Batch 020/110 | Loss: 0.10\n",
      "Epoch: 038/050 | Batch 040/110 | Loss: 0.22\n",
      "Epoch: 038/050 | Batch 060/110 | Loss: 0.16\n",
      "Epoch: 038/050 | Batch 080/110 | Loss: 0.06\n",
      "Epoch: 038/050 | Batch 100/110 | Loss: 0.20\n",
      "Epoch: 039/050 | Batch 000/110 | Loss: 0.16\n",
      "Epoch: 039/050 | Batch 020/110 | Loss: 0.10\n",
      "Epoch: 039/050 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 039/050 | Batch 060/110 | Loss: 0.16\n",
      "Epoch: 039/050 | Batch 080/110 | Loss: 0.12\n",
      "Epoch: 039/050 | Batch 100/110 | Loss: 0.19\n",
      "Epoch: 040/050 | Batch 000/110 | Loss: 0.23\n",
      "Epoch: 040/050 | Batch 020/110 | Loss: 0.12\n",
      "Epoch: 040/050 | Batch 040/110 | Loss: 0.15\n",
      "Epoch: 040/050 | Batch 060/110 | Loss: 0.23\n",
      "Epoch: 040/050 | Batch 080/110 | Loss: 0.11\n",
      "Epoch: 040/050 | Batch 100/110 | Loss: 0.17\n",
      "Epoch: 041/050 | Batch 000/110 | Loss: 0.12\n",
      "Epoch: 041/050 | Batch 020/110 | Loss: 0.10\n",
      "Epoch: 041/050 | Batch 040/110 | Loss: 0.11\n",
      "Epoch: 041/050 | Batch 060/110 | Loss: 0.15\n",
      "Epoch: 041/050 | Batch 080/110 | Loss: 0.06\n",
      "Epoch: 041/050 | Batch 100/110 | Loss: 0.19\n",
      "Epoch: 042/050 | Batch 000/110 | Loss: 0.14\n",
      "Epoch: 042/050 | Batch 020/110 | Loss: 0.22\n",
      "Epoch: 042/050 | Batch 040/110 | Loss: 0.11\n",
      "Epoch: 042/050 | Batch 060/110 | Loss: 0.10\n",
      "Epoch: 042/050 | Batch 080/110 | Loss: 0.07\n",
      "Epoch: 042/050 | Batch 100/110 | Loss: 0.16\n",
      "Epoch: 043/050 | Batch 000/110 | Loss: 0.10\n",
      "Epoch: 043/050 | Batch 020/110 | Loss: 0.13\n",
      "Epoch: 043/050 | Batch 040/110 | Loss: 0.30\n",
      "Epoch: 043/050 | Batch 060/110 | Loss: 0.20\n",
      "Epoch: 043/050 | Batch 080/110 | Loss: 0.13\n",
      "Epoch: 043/050 | Batch 100/110 | Loss: 0.27\n",
      "Epoch: 044/050 | Batch 000/110 | Loss: 0.14\n",
      "Epoch: 044/050 | Batch 020/110 | Loss: 0.36\n",
      "Epoch: 044/050 | Batch 040/110 | Loss: 0.29\n",
      "Epoch: 044/050 | Batch 060/110 | Loss: 0.11\n",
      "Epoch: 044/050 | Batch 080/110 | Loss: 0.04\n",
      "Epoch: 044/050 | Batch 100/110 | Loss: 0.20\n",
      "Epoch: 045/050 | Batch 000/110 | Loss: 0.27\n",
      "Epoch: 045/050 | Batch 020/110 | Loss: 0.32\n",
      "Epoch: 045/050 | Batch 040/110 | Loss: 0.15\n",
      "Epoch: 045/050 | Batch 060/110 | Loss: 0.15\n",
      "Epoch: 045/050 | Batch 080/110 | Loss: 0.05\n",
      "Epoch: 045/050 | Batch 100/110 | Loss: 0.18\n",
      "Epoch: 046/050 | Batch 000/110 | Loss: 0.11\n",
      "Epoch: 046/050 | Batch 020/110 | Loss: 0.15\n",
      "Epoch: 046/050 | Batch 040/110 | Loss: 0.07\n",
      "Epoch: 046/050 | Batch 060/110 | Loss: 0.27\n",
      "Epoch: 046/050 | Batch 080/110 | Loss: 0.08\n",
      "Epoch: 046/050 | Batch 100/110 | Loss: 0.14\n",
      "Epoch: 047/050 | Batch 000/110 | Loss: 0.12\n",
      "Epoch: 047/050 | Batch 020/110 | Loss: 0.09\n",
      "Epoch: 047/050 | Batch 040/110 | Loss: 0.23\n",
      "Epoch: 047/050 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 047/050 | Batch 080/110 | Loss: 0.28\n",
      "Epoch: 047/050 | Batch 100/110 | Loss: 0.11\n",
      "Epoch: 048/050 | Batch 000/110 | Loss: 0.11\n",
      "Epoch: 048/050 | Batch 020/110 | Loss: 0.26\n",
      "Epoch: 048/050 | Batch 040/110 | Loss: 0.13\n",
      "Epoch: 048/050 | Batch 060/110 | Loss: 0.10\n",
      "Epoch: 048/050 | Batch 080/110 | Loss: 0.18\n",
      "Epoch: 048/050 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 049/050 | Batch 000/110 | Loss: 0.38\n",
      "Epoch: 049/050 | Batch 020/110 | Loss: 0.11\n",
      "Epoch: 049/050 | Batch 040/110 | Loss: 0.15\n",
      "Epoch: 049/050 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 049/050 | Batch 080/110 | Loss: 0.22\n",
      "Epoch: 049/050 | Batch 100/110 | Loss: 0.12\n",
      "Epoch: 050/050 | Batch 000/110 | Loss: 0.10\n",
      "Epoch: 050/050 | Batch 020/110 | Loss: 0.10\n",
      "Epoch: 050/050 | Batch 040/110 | Loss: 0.16\n",
      "Epoch: 050/050 | Batch 060/110 | Loss: 0.14\n",
      "Epoch: 050/050 | Batch 080/110 | Loss: 0.10\n",
      "Epoch: 050/050 | Batch 100/110 | Loss: 0.11\n",
      "\n",
      "\n",
      "Learning rate: 0.002\n",
      "Epoch: 001/050 | Batch 000/110 | Loss: 1.34\n",
      "Epoch: 001/050 | Batch 020/110 | Loss: 1.16\n",
      "Epoch: 001/050 | Batch 040/110 | Loss: 1.08\n",
      "Epoch: 001/050 | Batch 060/110 | Loss: 0.58\n",
      "Epoch: 001/050 | Batch 080/110 | Loss: 1.38\n",
      "Epoch: 001/050 | Batch 100/110 | Loss: 0.58\n",
      "Epoch: 002/050 | Batch 000/110 | Loss: 0.63\n",
      "Epoch: 002/050 | Batch 020/110 | Loss: 0.98\n",
      "Epoch: 002/050 | Batch 040/110 | Loss: 0.66\n",
      "Epoch: 002/050 | Batch 060/110 | Loss: 0.56\n",
      "Epoch: 002/050 | Batch 080/110 | Loss: 0.55\n",
      "Epoch: 002/050 | Batch 100/110 | Loss: 0.47\n",
      "Epoch: 003/050 | Batch 000/110 | Loss: 0.52\n",
      "Epoch: 003/050 | Batch 020/110 | Loss: 0.33\n",
      "Epoch: 003/050 | Batch 040/110 | Loss: 0.33\n",
      "Epoch: 003/050 | Batch 060/110 | Loss: 0.20\n",
      "Epoch: 003/050 | Batch 080/110 | Loss: 0.39\n",
      "Epoch: 003/050 | Batch 100/110 | Loss: 0.48\n",
      "Epoch: 004/050 | Batch 000/110 | Loss: 0.22\n",
      "Epoch: 004/050 | Batch 020/110 | Loss: 0.40\n",
      "Epoch: 004/050 | Batch 040/110 | Loss: 0.25\n",
      "Epoch: 004/050 | Batch 060/110 | Loss: 0.35\n",
      "Epoch: 004/050 | Batch 080/110 | Loss: 0.24\n",
      "Epoch: 004/050 | Batch 100/110 | Loss: 0.58\n",
      "Epoch: 005/050 | Batch 000/110 | Loss: 0.44\n",
      "Epoch: 005/050 | Batch 020/110 | Loss: 0.34\n",
      "Epoch: 005/050 | Batch 040/110 | Loss: 0.30\n",
      "Epoch: 005/050 | Batch 060/110 | Loss: 0.11\n",
      "Epoch: 005/050 | Batch 080/110 | Loss: 0.18\n",
      "Epoch: 005/050 | Batch 100/110 | Loss: 0.15\n",
      "Epoch: 006/050 | Batch 000/110 | Loss: 0.28\n",
      "Epoch: 006/050 | Batch 020/110 | Loss: 0.17\n",
      "Epoch: 006/050 | Batch 040/110 | Loss: 0.35\n",
      "Epoch: 006/050 | Batch 060/110 | Loss: 0.32\n",
      "Epoch: 006/050 | Batch 080/110 | Loss: 0.19\n",
      "Epoch: 006/050 | Batch 100/110 | Loss: 0.33\n",
      "Epoch: 007/050 | Batch 000/110 | Loss: 0.16\n",
      "Epoch: 007/050 | Batch 020/110 | Loss: 0.18\n",
      "Epoch: 007/050 | Batch 040/110 | Loss: 0.19\n",
      "Epoch: 007/050 | Batch 060/110 | Loss: 0.16\n",
      "Epoch: 007/050 | Batch 080/110 | Loss: 0.18\n",
      "Epoch: 007/050 | Batch 100/110 | Loss: 0.19\n",
      "Epoch: 008/050 | Batch 000/110 | Loss: 0.22\n",
      "Epoch: 008/050 | Batch 020/110 | Loss: 0.07\n",
      "Epoch: 008/050 | Batch 040/110 | Loss: 0.08\n",
      "Epoch: 008/050 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 008/050 | Batch 080/110 | Loss: 0.15\n",
      "Epoch: 008/050 | Batch 100/110 | Loss: 0.20\n",
      "Epoch: 009/050 | Batch 000/110 | Loss: 0.15\n",
      "Epoch: 009/050 | Batch 020/110 | Loss: 0.20\n",
      "Epoch: 009/050 | Batch 040/110 | Loss: 0.20\n",
      "Epoch: 009/050 | Batch 060/110 | Loss: 0.17\n",
      "Epoch: 009/050 | Batch 080/110 | Loss: 0.09\n",
      "Epoch: 009/050 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 010/050 | Batch 000/110 | Loss: 0.19\n",
      "Epoch: 010/050 | Batch 020/110 | Loss: 0.23\n",
      "Epoch: 010/050 | Batch 040/110 | Loss: 0.34\n",
      "Epoch: 010/050 | Batch 060/110 | Loss: 0.18\n",
      "Epoch: 010/050 | Batch 080/110 | Loss: 0.25\n",
      "Epoch: 010/050 | Batch 100/110 | Loss: 0.16\n",
      "Epoch: 011/050 | Batch 000/110 | Loss: 0.11\n",
      "Epoch: 011/050 | Batch 020/110 | Loss: 0.13\n",
      "Epoch: 011/050 | Batch 040/110 | Loss: 0.14\n",
      "Epoch: 011/050 | Batch 060/110 | Loss: 0.09\n",
      "Epoch: 011/050 | Batch 080/110 | Loss: 0.19\n",
      "Epoch: 011/050 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 012/050 | Batch 000/110 | Loss: 0.24\n",
      "Epoch: 012/050 | Batch 020/110 | Loss: 0.28\n",
      "Epoch: 012/050 | Batch 040/110 | Loss: 0.09\n",
      "Epoch: 012/050 | Batch 060/110 | Loss: 0.22\n",
      "Epoch: 012/050 | Batch 080/110 | Loss: 0.26\n",
      "Epoch: 012/050 | Batch 100/110 | Loss: 0.15\n",
      "Epoch: 013/050 | Batch 000/110 | Loss: 0.08\n",
      "Epoch: 013/050 | Batch 020/110 | Loss: 0.13\n",
      "Epoch: 013/050 | Batch 040/110 | Loss: 0.31\n",
      "Epoch: 013/050 | Batch 060/110 | Loss: 0.18\n",
      "Epoch: 013/050 | Batch 080/110 | Loss: 0.26\n",
      "Epoch: 013/050 | Batch 100/110 | Loss: 0.30\n",
      "Epoch: 014/050 | Batch 000/110 | Loss: 0.30\n",
      "Epoch: 014/050 | Batch 020/110 | Loss: 0.16\n",
      "Epoch: 014/050 | Batch 040/110 | Loss: 0.11\n",
      "Epoch: 014/050 | Batch 060/110 | Loss: 0.16\n",
      "Epoch: 014/050 | Batch 080/110 | Loss: 0.09\n",
      "Epoch: 014/050 | Batch 100/110 | Loss: 0.11\n",
      "Epoch: 015/050 | Batch 000/110 | Loss: 0.22\n",
      "Epoch: 015/050 | Batch 020/110 | Loss: 0.16\n",
      "Epoch: 015/050 | Batch 040/110 | Loss: 0.19\n",
      "Epoch: 015/050 | Batch 060/110 | Loss: 0.21\n",
      "Epoch: 015/050 | Batch 080/110 | Loss: 0.13\n",
      "Epoch: 015/050 | Batch 100/110 | Loss: 0.07\n",
      "Epoch: 016/050 | Batch 000/110 | Loss: 0.18\n",
      "Epoch: 016/050 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 016/050 | Batch 040/110 | Loss: 0.23\n",
      "Epoch: 016/050 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 016/050 | Batch 080/110 | Loss: 0.23\n",
      "Epoch: 016/050 | Batch 100/110 | Loss: 0.05\n",
      "Epoch: 017/050 | Batch 000/110 | Loss: 0.20\n",
      "Epoch: 017/050 | Batch 020/110 | Loss: 0.09\n",
      "Epoch: 017/050 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 017/050 | Batch 060/110 | Loss: 0.16\n",
      "Epoch: 017/050 | Batch 080/110 | Loss: 0.21\n",
      "Epoch: 017/050 | Batch 100/110 | Loss: 0.07\n",
      "Epoch: 018/050 | Batch 000/110 | Loss: 0.18\n",
      "Epoch: 018/050 | Batch 020/110 | Loss: 0.20\n",
      "Epoch: 018/050 | Batch 040/110 | Loss: 0.19\n",
      "Epoch: 018/050 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 018/050 | Batch 080/110 | Loss: 0.23\n",
      "Epoch: 018/050 | Batch 100/110 | Loss: 0.32\n",
      "Epoch: 019/050 | Batch 000/110 | Loss: 0.13\n",
      "Epoch: 019/050 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 019/050 | Batch 040/110 | Loss: 0.16\n",
      "Epoch: 019/050 | Batch 060/110 | Loss: 0.18\n",
      "Epoch: 019/050 | Batch 080/110 | Loss: 0.19\n",
      "Epoch: 019/050 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 020/050 | Batch 000/110 | Loss: 0.05\n",
      "Epoch: 020/050 | Batch 020/110 | Loss: 0.14\n",
      "Epoch: 020/050 | Batch 040/110 | Loss: 0.22\n",
      "Epoch: 020/050 | Batch 060/110 | Loss: 0.24\n",
      "Epoch: 020/050 | Batch 080/110 | Loss: 0.11\n",
      "Epoch: 020/050 | Batch 100/110 | Loss: 0.06\n",
      "Epoch: 021/050 | Batch 000/110 | Loss: 0.20\n",
      "Epoch: 021/050 | Batch 020/110 | Loss: 0.12\n",
      "Epoch: 021/050 | Batch 040/110 | Loss: 0.09\n",
      "Epoch: 021/050 | Batch 060/110 | Loss: 0.24\n",
      "Epoch: 021/050 | Batch 080/110 | Loss: 0.24\n",
      "Epoch: 021/050 | Batch 100/110 | Loss: 0.12\n",
      "Epoch: 022/050 | Batch 000/110 | Loss: 0.05\n",
      "Epoch: 022/050 | Batch 020/110 | Loss: 0.26\n",
      "Epoch: 022/050 | Batch 040/110 | Loss: 0.15\n",
      "Epoch: 022/050 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 022/050 | Batch 080/110 | Loss: 0.07\n",
      "Epoch: 022/050 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 023/050 | Batch 000/110 | Loss: 0.08\n",
      "Epoch: 023/050 | Batch 020/110 | Loss: 0.16\n",
      "Epoch: 023/050 | Batch 040/110 | Loss: 0.19\n",
      "Epoch: 023/050 | Batch 060/110 | Loss: 0.18\n",
      "Epoch: 023/050 | Batch 080/110 | Loss: 0.09\n",
      "Epoch: 023/050 | Batch 100/110 | Loss: 0.13\n",
      "Epoch: 024/050 | Batch 000/110 | Loss: 0.17\n",
      "Epoch: 024/050 | Batch 020/110 | Loss: 0.10\n",
      "Epoch: 024/050 | Batch 040/110 | Loss: 0.31\n",
      "Epoch: 024/050 | Batch 060/110 | Loss: 0.11\n",
      "Epoch: 024/050 | Batch 080/110 | Loss: 0.06\n",
      "Epoch: 024/050 | Batch 100/110 | Loss: 0.24\n",
      "Epoch: 025/050 | Batch 000/110 | Loss: 0.04\n",
      "Epoch: 025/050 | Batch 020/110 | Loss: 0.05\n",
      "Epoch: 025/050 | Batch 040/110 | Loss: 0.03\n",
      "Epoch: 025/050 | Batch 060/110 | Loss: 0.02\n",
      "Epoch: 025/050 | Batch 080/110 | Loss: 0.11\n",
      "Epoch: 025/050 | Batch 100/110 | Loss: 0.08\n",
      "Epoch: 026/050 | Batch 000/110 | Loss: 0.08\n",
      "Epoch: 026/050 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 026/050 | Batch 040/110 | Loss: 0.04\n",
      "Epoch: 026/050 | Batch 060/110 | Loss: 0.18\n",
      "Epoch: 026/050 | Batch 080/110 | Loss: 0.20\n",
      "Epoch: 026/050 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 027/050 | Batch 000/110 | Loss: 0.07\n",
      "Epoch: 027/050 | Batch 020/110 | Loss: 0.11\n",
      "Epoch: 027/050 | Batch 040/110 | Loss: 0.09\n",
      "Epoch: 027/050 | Batch 060/110 | Loss: 0.12\n",
      "Epoch: 027/050 | Batch 080/110 | Loss: 0.11\n",
      "Epoch: 027/050 | Batch 100/110 | Loss: 0.05\n",
      "Epoch: 028/050 | Batch 000/110 | Loss: 0.10\n",
      "Epoch: 028/050 | Batch 020/110 | Loss: 0.14\n",
      "Epoch: 028/050 | Batch 040/110 | Loss: 0.09\n",
      "Epoch: 028/050 | Batch 060/110 | Loss: 0.12\n",
      "Epoch: 028/050 | Batch 080/110 | Loss: 0.10\n",
      "Epoch: 028/050 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 029/050 | Batch 000/110 | Loss: 0.13\n",
      "Epoch: 029/050 | Batch 020/110 | Loss: 0.10\n",
      "Epoch: 029/050 | Batch 040/110 | Loss: 0.24\n",
      "Epoch: 029/050 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 029/050 | Batch 080/110 | Loss: 0.13\n",
      "Epoch: 029/050 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 030/050 | Batch 000/110 | Loss: 0.11\n",
      "Epoch: 030/050 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 030/050 | Batch 040/110 | Loss: 0.05\n",
      "Epoch: 030/050 | Batch 060/110 | Loss: 0.04\n",
      "Epoch: 030/050 | Batch 080/110 | Loss: 0.03\n",
      "Epoch: 030/050 | Batch 100/110 | Loss: 0.18\n",
      "Epoch: 031/050 | Batch 000/110 | Loss: 0.03\n",
      "Epoch: 031/050 | Batch 020/110 | Loss: 0.09\n",
      "Epoch: 031/050 | Batch 040/110 | Loss: 0.09\n",
      "Epoch: 031/050 | Batch 060/110 | Loss: 0.14\n",
      "Epoch: 031/050 | Batch 080/110 | Loss: 0.13\n",
      "Epoch: 031/050 | Batch 100/110 | Loss: 0.29\n",
      "Epoch: 032/050 | Batch 000/110 | Loss: 0.04\n",
      "Epoch: 032/050 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 032/050 | Batch 040/110 | Loss: 0.09\n",
      "Epoch: 032/050 | Batch 060/110 | Loss: 0.18\n",
      "Epoch: 032/050 | Batch 080/110 | Loss: 0.14\n",
      "Epoch: 032/050 | Batch 100/110 | Loss: 0.11\n",
      "Epoch: 033/050 | Batch 000/110 | Loss: 0.21\n",
      "Epoch: 033/050 | Batch 020/110 | Loss: 0.09\n",
      "Epoch: 033/050 | Batch 040/110 | Loss: 0.11\n",
      "Epoch: 033/050 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 033/050 | Batch 080/110 | Loss: 0.12\n",
      "Epoch: 033/050 | Batch 100/110 | Loss: 0.12\n",
      "Epoch: 034/050 | Batch 000/110 | Loss: 0.33\n",
      "Epoch: 034/050 | Batch 020/110 | Loss: 0.20\n",
      "Epoch: 034/050 | Batch 040/110 | Loss: 0.05\n",
      "Epoch: 034/050 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 034/050 | Batch 080/110 | Loss: 0.06\n",
      "Epoch: 034/050 | Batch 100/110 | Loss: 0.11\n",
      "Epoch: 035/050 | Batch 000/110 | Loss: 0.23\n",
      "Epoch: 035/050 | Batch 020/110 | Loss: 0.12\n",
      "Epoch: 035/050 | Batch 040/110 | Loss: 0.15\n",
      "Reached desired loss, stopping training.\n",
      "\n",
      "\n",
      "Learning rate: 0.003\n",
      "Epoch: 001/050 | Batch 000/110 | Loss: 0.62\n",
      "Epoch: 001/050 | Batch 020/110 | Loss: 0.39\n",
      "Epoch: 001/050 | Batch 040/110 | Loss: 0.41\n",
      "Epoch: 001/050 | Batch 060/110 | Loss: 0.51\n",
      "Epoch: 001/050 | Batch 080/110 | Loss: 0.23\n",
      "Epoch: 001/050 | Batch 100/110 | Loss: 0.29\n",
      "Epoch: 002/050 | Batch 000/110 | Loss: 0.38\n",
      "Epoch: 002/050 | Batch 020/110 | Loss: 0.35\n",
      "Epoch: 002/050 | Batch 040/110 | Loss: 0.32\n",
      "Epoch: 002/050 | Batch 060/110 | Loss: 0.22\n",
      "Epoch: 002/050 | Batch 080/110 | Loss: 0.16\n",
      "Epoch: 002/050 | Batch 100/110 | Loss: 0.23\n",
      "Epoch: 003/050 | Batch 000/110 | Loss: 0.21\n",
      "Epoch: 003/050 | Batch 020/110 | Loss: 0.15\n",
      "Epoch: 003/050 | Batch 040/110 | Loss: 0.17\n",
      "Epoch: 003/050 | Batch 060/110 | Loss: 0.16\n",
      "Epoch: 003/050 | Batch 080/110 | Loss: 0.22\n",
      "Epoch: 003/050 | Batch 100/110 | Loss: 0.21\n",
      "Epoch: 004/050 | Batch 000/110 | Loss: 0.15\n",
      "Epoch: 004/050 | Batch 020/110 | Loss: 0.22\n",
      "Epoch: 004/050 | Batch 040/110 | Loss: 0.14\n",
      "Epoch: 004/050 | Batch 060/110 | Loss: 0.23\n",
      "Epoch: 004/050 | Batch 080/110 | Loss: 0.17\n",
      "Epoch: 004/050 | Batch 100/110 | Loss: 0.40\n",
      "Epoch: 005/050 | Batch 000/110 | Loss: 0.28\n",
      "Epoch: 005/050 | Batch 020/110 | Loss: 0.22\n",
      "Epoch: 005/050 | Batch 040/110 | Loss: 0.23\n",
      "Epoch: 005/050 | Batch 060/110 | Loss: 0.07\n",
      "Epoch: 005/050 | Batch 080/110 | Loss: 0.14\n",
      "Epoch: 005/050 | Batch 100/110 | Loss: 0.14\n",
      "Epoch: 006/050 | Batch 000/110 | Loss: 0.16\n",
      "Epoch: 006/050 | Batch 020/110 | Loss: 0.14\n",
      "Epoch: 006/050 | Batch 040/110 | Loss: 0.26\n",
      "Epoch: 006/050 | Batch 060/110 | Loss: 0.17\n",
      "Epoch: 006/050 | Batch 080/110 | Loss: 0.14\n",
      "Epoch: 006/050 | Batch 100/110 | Loss: 0.14\n",
      "Epoch: 007/050 | Batch 000/110 | Loss: 0.10\n",
      "Epoch: 007/050 | Batch 020/110 | Loss: 0.10\n",
      "Epoch: 007/050 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 007/050 | Batch 060/110 | Loss: 0.12\n",
      "Epoch: 007/050 | Batch 080/110 | Loss: 0.13\n",
      "Epoch: 007/050 | Batch 100/110 | Loss: 0.14\n",
      "Epoch: 008/050 | Batch 000/110 | Loss: 0.14\n",
      "Epoch: 008/050 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 008/050 | Batch 040/110 | Loss: 0.03\n",
      "Epoch: 008/050 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 008/050 | Batch 080/110 | Loss: 0.10\n",
      "Epoch: 008/050 | Batch 100/110 | Loss: 0.11\n",
      "Epoch: 009/050 | Batch 000/110 | Loss: 0.07\n",
      "Epoch: 009/050 | Batch 020/110 | Loss: 0.18\n",
      "Epoch: 009/050 | Batch 040/110 | Loss: 0.15\n",
      "Epoch: 009/050 | Batch 060/110 | Loss: 0.12\n",
      "Epoch: 009/050 | Batch 080/110 | Loss: 0.06\n",
      "Epoch: 009/050 | Batch 100/110 | Loss: 0.08\n",
      "Epoch: 010/050 | Batch 000/110 | Loss: 0.14\n",
      "Epoch: 010/050 | Batch 020/110 | Loss: 0.14\n",
      "Epoch: 010/050 | Batch 040/110 | Loss: 0.30\n",
      "Epoch: 010/050 | Batch 060/110 | Loss: 0.16\n",
      "Epoch: 010/050 | Batch 080/110 | Loss: 0.24\n",
      "Epoch: 010/050 | Batch 100/110 | Loss: 0.11\n",
      "Epoch: 011/050 | Batch 000/110 | Loss: 0.08\n",
      "Epoch: 011/050 | Batch 020/110 | Loss: 0.09\n",
      "Epoch: 011/050 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 011/050 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 011/050 | Batch 080/110 | Loss: 0.16\n",
      "Epoch: 011/050 | Batch 100/110 | Loss: 0.07\n",
      "Epoch: 012/050 | Batch 000/110 | Loss: 0.21\n",
      "Epoch: 012/050 | Batch 020/110 | Loss: 0.21\n",
      "Epoch: 012/050 | Batch 040/110 | Loss: 0.08\n",
      "Epoch: 012/050 | Batch 060/110 | Loss: 0.21\n",
      "Epoch: 012/050 | Batch 080/110 | Loss: 0.19\n",
      "Epoch: 012/050 | Batch 100/110 | Loss: 0.13\n",
      "Epoch: 013/050 | Batch 000/110 | Loss: 0.06\n",
      "Epoch: 013/050 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 013/050 | Batch 040/110 | Loss: 0.28\n",
      "Epoch: 013/050 | Batch 060/110 | Loss: 0.16\n",
      "Epoch: 013/050 | Batch 080/110 | Loss: 0.22\n",
      "Epoch: 013/050 | Batch 100/110 | Loss: 0.29\n",
      "Epoch: 014/050 | Batch 000/110 | Loss: 0.30\n",
      "Epoch: 014/050 | Batch 020/110 | Loss: 0.12\n",
      "Epoch: 014/050 | Batch 040/110 | Loss: 0.08\n",
      "Epoch: 014/050 | Batch 060/110 | Loss: 0.12\n",
      "Epoch: 014/050 | Batch 080/110 | Loss: 0.06\n",
      "Epoch: 014/050 | Batch 100/110 | Loss: 0.09\n",
      "Epoch: 015/050 | Batch 000/110 | Loss: 0.18\n",
      "Epoch: 015/050 | Batch 020/110 | Loss: 0.12\n",
      "Epoch: 015/050 | Batch 040/110 | Loss: 0.13\n",
      "Epoch: 015/050 | Batch 060/110 | Loss: 0.17\n",
      "Epoch: 015/050 | Batch 080/110 | Loss: 0.12\n",
      "Epoch: 015/050 | Batch 100/110 | Loss: 0.05\n",
      "Epoch: 016/050 | Batch 000/110 | Loss: 0.15\n",
      "Epoch: 016/050 | Batch 020/110 | Loss: 0.02\n",
      "Epoch: 016/050 | Batch 040/110 | Loss: 0.19\n",
      "Epoch: 016/050 | Batch 060/110 | Loss: 0.09\n",
      "Epoch: 016/050 | Batch 080/110 | Loss: 0.22\n",
      "Epoch: 016/050 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 017/050 | Batch 000/110 | Loss: 0.21\n",
      "Epoch: 017/050 | Batch 020/110 | Loss: 0.06\n",
      "Epoch: 017/050 | Batch 040/110 | Loss: 0.09\n",
      "Epoch: 017/050 | Batch 060/110 | Loss: 0.14\n",
      "Epoch: 017/050 | Batch 080/110 | Loss: 0.19\n",
      "Epoch: 017/050 | Batch 100/110 | Loss: 0.06\n",
      "Epoch: 018/050 | Batch 000/110 | Loss: 0.13\n",
      "Epoch: 018/050 | Batch 020/110 | Loss: 0.16\n",
      "Epoch: 018/050 | Batch 040/110 | Loss: 0.16\n",
      "Reached desired loss, stopping training.\n",
      "\n",
      "\n",
      "Learning rate: 0.004\n",
      "Epoch: 001/050 | Batch 000/110 | Loss: 1.30\n",
      "Epoch: 001/050 | Batch 020/110 | Loss: 0.91\n",
      "Epoch: 001/050 | Batch 040/110 | Loss: 0.78\n",
      "Epoch: 001/050 | Batch 060/110 | Loss: 0.56\n",
      "Epoch: 001/050 | Batch 080/110 | Loss: 0.62\n",
      "Epoch: 001/050 | Batch 100/110 | Loss: 0.40\n",
      "Epoch: 002/050 | Batch 000/110 | Loss: 0.44\n",
      "Epoch: 002/050 | Batch 020/110 | Loss: 0.48\n",
      "Epoch: 002/050 | Batch 040/110 | Loss: 0.43\n",
      "Epoch: 002/050 | Batch 060/110 | Loss: 0.32\n",
      "Epoch: 002/050 | Batch 080/110 | Loss: 0.26\n",
      "Epoch: 002/050 | Batch 100/110 | Loss: 0.29\n",
      "Epoch: 003/050 | Batch 000/110 | Loss: 0.29\n",
      "Epoch: 003/050 | Batch 020/110 | Loss: 0.19\n",
      "Epoch: 003/050 | Batch 040/110 | Loss: 0.22\n",
      "Epoch: 003/050 | Batch 060/110 | Loss: 0.17\n",
      "Epoch: 003/050 | Batch 080/110 | Loss: 0.26\n",
      "Epoch: 003/050 | Batch 100/110 | Loss: 0.25\n",
      "Epoch: 004/050 | Batch 000/110 | Loss: 0.17\n",
      "Epoch: 004/050 | Batch 020/110 | Loss: 0.25\n",
      "Epoch: 004/050 | Batch 040/110 | Loss: 0.16\n",
      "Epoch: 004/050 | Batch 060/110 | Loss: 0.25\n",
      "Epoch: 004/050 | Batch 080/110 | Loss: 0.18\n",
      "Epoch: 004/050 | Batch 100/110 | Loss: 0.43\n",
      "Epoch: 005/050 | Batch 000/110 | Loss: 0.30\n",
      "Epoch: 005/050 | Batch 020/110 | Loss: 0.24\n",
      "Epoch: 005/050 | Batch 040/110 | Loss: 0.23\n",
      "Epoch: 005/050 | Batch 060/110 | Loss: 0.07\n",
      "Epoch: 005/050 | Batch 080/110 | Loss: 0.14\n",
      "Epoch: 005/050 | Batch 100/110 | Loss: 0.13\n",
      "Epoch: 006/050 | Batch 000/110 | Loss: 0.17\n",
      "Epoch: 006/050 | Batch 020/110 | Loss: 0.13\n",
      "Epoch: 006/050 | Batch 040/110 | Loss: 0.28\n",
      "Epoch: 006/050 | Batch 060/110 | Loss: 0.19\n",
      "Epoch: 006/050 | Batch 080/110 | Loss: 0.14\n",
      "Epoch: 006/050 | Batch 100/110 | Loss: 0.16\n",
      "Epoch: 007/050 | Batch 000/110 | Loss: 0.10\n",
      "Epoch: 007/050 | Batch 020/110 | Loss: 0.11\n",
      "Epoch: 007/050 | Batch 040/110 | Loss: 0.13\n",
      "Epoch: 007/050 | Batch 060/110 | Loss: 0.11\n",
      "Epoch: 007/050 | Batch 080/110 | Loss: 0.13\n",
      "Epoch: 007/050 | Batch 100/110 | Loss: 0.13\n",
      "Epoch: 008/050 | Batch 000/110 | Loss: 0.15\n",
      "Epoch: 008/050 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 008/050 | Batch 040/110 | Loss: 0.03\n",
      "Epoch: 008/050 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 008/050 | Batch 080/110 | Loss: 0.10\n",
      "Epoch: 008/050 | Batch 100/110 | Loss: 0.12\n",
      "Epoch: 009/050 | Batch 000/110 | Loss: 0.08\n",
      "Epoch: 009/050 | Batch 020/110 | Loss: 0.17\n",
      "Epoch: 009/050 | Batch 040/110 | Loss: 0.14\n",
      "Epoch: 009/050 | Batch 060/110 | Loss: 0.11\n",
      "Epoch: 009/050 | Batch 080/110 | Loss: 0.06\n",
      "Epoch: 009/050 | Batch 100/110 | Loss: 0.07\n",
      "Epoch: 010/050 | Batch 000/110 | Loss: 0.14\n",
      "Epoch: 010/050 | Batch 020/110 | Loss: 0.14\n",
      "Epoch: 010/050 | Batch 040/110 | Loss: 0.30\n",
      "Epoch: 010/050 | Batch 060/110 | Loss: 0.15\n",
      "Epoch: 010/050 | Batch 080/110 | Loss: 0.22\n",
      "Epoch: 010/050 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 011/050 | Batch 000/110 | Loss: 0.07\n",
      "Epoch: 011/050 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 011/050 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 011/050 | Batch 060/110 | Loss: 0.05\n",
      "Epoch: 011/050 | Batch 080/110 | Loss: 0.16\n",
      "Epoch: 011/050 | Batch 100/110 | Loss: 0.07\n",
      "Epoch: 012/050 | Batch 000/110 | Loss: 0.21\n",
      "Epoch: 012/050 | Batch 020/110 | Loss: 0.20\n",
      "Epoch: 012/050 | Batch 040/110 | Loss: 0.07\n",
      "Epoch: 012/050 | Batch 060/110 | Loss: 0.19\n",
      "Epoch: 012/050 | Batch 080/110 | Loss: 0.20\n",
      "Epoch: 012/050 | Batch 100/110 | Loss: 0.12\n",
      "Epoch: 013/050 | Batch 000/110 | Loss: 0.05\n",
      "Epoch: 013/050 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 013/050 | Batch 040/110 | Loss: 0.27\n",
      "Epoch: 013/050 | Batch 060/110 | Loss: 0.15\n",
      "Epoch: 013/050 | Batch 080/110 | Loss: 0.22\n",
      "Epoch: 013/050 | Batch 100/110 | Loss: 0.28\n",
      "Epoch: 014/050 | Batch 000/110 | Loss: 0.28\n",
      "Epoch: 014/050 | Batch 020/110 | Loss: 0.11\n",
      "Epoch: 014/050 | Batch 040/110 | Loss: 0.08\n",
      "Epoch: 014/050 | Batch 060/110 | Loss: 0.11\n",
      "Epoch: 014/050 | Batch 080/110 | Loss: 0.05\n",
      "Epoch: 014/050 | Batch 100/110 | Loss: 0.09\n",
      "Epoch: 015/050 | Batch 000/110 | Loss: 0.17\n",
      "Epoch: 015/050 | Batch 020/110 | Loss: 0.12\n",
      "Epoch: 015/050 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 015/050 | Batch 060/110 | Loss: 0.17\n",
      "Epoch: 015/050 | Batch 080/110 | Loss: 0.11\n",
      "Epoch: 015/050 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 016/050 | Batch 000/110 | Loss: 0.15\n",
      "Epoch: 016/050 | Batch 020/110 | Loss: 0.02\n",
      "Epoch: 016/050 | Batch 040/110 | Loss: 0.19\n",
      "Epoch: 016/050 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 016/050 | Batch 080/110 | Loss: 0.21\n",
      "Epoch: 016/050 | Batch 100/110 | Loss: 0.03\n",
      "Epoch: 017/050 | Batch 000/110 | Loss: 0.20\n",
      "Epoch: 017/050 | Batch 020/110 | Loss: 0.05\n",
      "Epoch: 017/050 | Batch 040/110 | Loss: 0.08\n",
      "Epoch: 017/050 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 017/050 | Batch 080/110 | Loss: 0.18\n",
      "Epoch: 017/050 | Batch 100/110 | Loss: 0.05\n",
      "Epoch: 018/050 | Batch 000/110 | Loss: 0.13\n",
      "Epoch: 018/050 | Batch 020/110 | Loss: 0.16\n",
      "Epoch: 018/050 | Batch 040/110 | Loss: 0.15\n",
      "Reached desired loss, stopping training.\n",
      "\n",
      "\n",
      "Learning rate: 0.005\n",
      "Epoch: 001/050 | Batch 000/110 | Loss: 1.30\n",
      "Epoch: 001/050 | Batch 020/110 | Loss: 0.87\n",
      "Epoch: 001/050 | Batch 040/110 | Loss: 0.72\n",
      "Epoch: 001/050 | Batch 060/110 | Loss: 0.52\n",
      "Epoch: 001/050 | Batch 080/110 | Loss: 0.53\n",
      "Epoch: 001/050 | Batch 100/110 | Loss: 0.36\n",
      "Epoch: 002/050 | Batch 000/110 | Loss: 0.40\n",
      "Epoch: 002/050 | Batch 020/110 | Loss: 0.43\n",
      "Epoch: 002/050 | Batch 040/110 | Loss: 0.39\n",
      "Epoch: 002/050 | Batch 060/110 | Loss: 0.29\n",
      "Epoch: 002/050 | Batch 080/110 | Loss: 0.22\n",
      "Epoch: 002/050 | Batch 100/110 | Loss: 0.25\n",
      "Epoch: 003/050 | Batch 000/110 | Loss: 0.26\n",
      "Epoch: 003/050 | Batch 020/110 | Loss: 0.16\n",
      "Epoch: 003/050 | Batch 040/110 | Loss: 0.19\n",
      "Epoch: 003/050 | Batch 060/110 | Loss: 0.15\n",
      "Epoch: 003/050 | Batch 080/110 | Loss: 0.22\n",
      "Epoch: 003/050 | Batch 100/110 | Loss: 0.22\n",
      "Epoch: 004/050 | Batch 000/110 | Loss: 0.14\n",
      "Epoch: 004/050 | Batch 020/110 | Loss: 0.22\n",
      "Epoch: 004/050 | Batch 040/110 | Loss: 0.14\n",
      "Epoch: 004/050 | Batch 060/110 | Loss: 0.24\n",
      "Epoch: 004/050 | Batch 080/110 | Loss: 0.15\n",
      "Epoch: 004/050 | Batch 100/110 | Loss: 0.42\n",
      "Epoch: 005/050 | Batch 000/110 | Loss: 0.28\n",
      "Epoch: 005/050 | Batch 020/110 | Loss: 0.21\n",
      "Epoch: 005/050 | Batch 040/110 | Loss: 0.20\n",
      "Epoch: 005/050 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 005/050 | Batch 080/110 | Loss: 0.12\n",
      "Epoch: 005/050 | Batch 100/110 | Loss: 0.12\n",
      "Epoch: 006/050 | Batch 000/110 | Loss: 0.15\n",
      "Epoch: 006/050 | Batch 020/110 | Loss: 0.11\n",
      "Epoch: 006/050 | Batch 040/110 | Loss: 0.26\n",
      "Epoch: 006/050 | Batch 060/110 | Loss: 0.17\n",
      "Epoch: 006/050 | Batch 080/110 | Loss: 0.13\n",
      "Epoch: 006/050 | Batch 100/110 | Loss: 0.14\n",
      "Epoch: 007/050 | Batch 000/110 | Loss: 0.08\n",
      "Epoch: 007/050 | Batch 020/110 | Loss: 0.09\n",
      "Epoch: 007/050 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 007/050 | Batch 060/110 | Loss: 0.09\n",
      "Epoch: 007/050 | Batch 080/110 | Loss: 0.12\n",
      "Epoch: 007/050 | Batch 100/110 | Loss: 0.12\n",
      "Epoch: 008/050 | Batch 000/110 | Loss: 0.13\n",
      "Epoch: 008/050 | Batch 020/110 | Loss: 0.03\n",
      "Epoch: 008/050 | Batch 040/110 | Loss: 0.02\n",
      "Epoch: 008/050 | Batch 060/110 | Loss: 0.07\n",
      "Epoch: 008/050 | Batch 080/110 | Loss: 0.09\n",
      "Epoch: 008/050 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 009/050 | Batch 000/110 | Loss: 0.07\n",
      "Epoch: 009/050 | Batch 020/110 | Loss: 0.16\n",
      "Epoch: 009/050 | Batch 040/110 | Loss: 0.13\n",
      "Epoch: 009/050 | Batch 060/110 | Loss: 0.10\n",
      "Epoch: 009/050 | Batch 080/110 | Loss: 0.05\n",
      "Epoch: 009/050 | Batch 100/110 | Loss: 0.06\n",
      "Epoch: 010/050 | Batch 000/110 | Loss: 0.12\n",
      "Epoch: 010/050 | Batch 020/110 | Loss: 0.13\n",
      "Epoch: 010/050 | Batch 040/110 | Loss: 0.29\n",
      "Epoch: 010/050 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 010/050 | Batch 080/110 | Loss: 0.19\n",
      "Epoch: 010/050 | Batch 100/110 | Loss: 0.09\n",
      "Epoch: 011/050 | Batch 000/110 | Loss: 0.06\n",
      "Epoch: 011/050 | Batch 020/110 | Loss: 0.07\n",
      "Epoch: 011/050 | Batch 040/110 | Loss: 0.11\n",
      "Epoch: 011/050 | Batch 060/110 | Loss: 0.04\n",
      "Epoch: 011/050 | Batch 080/110 | Loss: 0.15\n",
      "Epoch: 011/050 | Batch 100/110 | Loss: 0.06\n",
      "Epoch: 012/050 | Batch 000/110 | Loss: 0.20\n",
      "Epoch: 012/050 | Batch 020/110 | Loss: 0.19\n",
      "Epoch: 012/050 | Batch 040/110 | Loss: 0.06\n",
      "Epoch: 012/050 | Batch 060/110 | Loss: 0.18\n",
      "Epoch: 012/050 | Batch 080/110 | Loss: 0.19\n",
      "Epoch: 012/050 | Batch 100/110 | Loss: 0.11\n",
      "Epoch: 013/050 | Batch 000/110 | Loss: 0.05\n",
      "Epoch: 013/050 | Batch 020/110 | Loss: 0.07\n",
      "Epoch: 013/050 | Batch 040/110 | Loss: 0.25\n",
      "Epoch: 013/050 | Batch 060/110 | Loss: 0.14\n",
      "Epoch: 013/050 | Batch 080/110 | Loss: 0.21\n",
      "Epoch: 013/050 | Batch 100/110 | Loss: 0.28\n",
      "Epoch: 014/050 | Batch 000/110 | Loss: 0.25\n",
      "Epoch: 014/050 | Batch 020/110 | Loss: 0.09\n",
      "Epoch: 014/050 | Batch 040/110 | Loss: 0.07\n",
      "Epoch: 014/050 | Batch 060/110 | Loss: 0.10\n",
      "Epoch: 014/050 | Batch 080/110 | Loss: 0.04\n",
      "Epoch: 014/050 | Batch 100/110 | Loss: 0.08\n",
      "Epoch: 015/050 | Batch 000/110 | Loss: 0.15\n",
      "Epoch: 015/050 | Batch 020/110 | Loss: 0.10\n",
      "Epoch: 015/050 | Batch 040/110 | Loss: 0.11\n",
      "Epoch: 015/050 | Batch 060/110 | Loss: 0.16\n",
      "Epoch: 015/050 | Batch 080/110 | Loss: 0.09\n",
      "Epoch: 015/050 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 016/050 | Batch 000/110 | Loss: 0.14\n",
      "Epoch: 016/050 | Batch 020/110 | Loss: 0.02\n",
      "Epoch: 016/050 | Batch 040/110 | Loss: 0.18\n",
      "Epoch: 016/050 | Batch 060/110 | Loss: 0.07\n",
      "Epoch: 016/050 | Batch 080/110 | Loss: 0.20\n",
      "Epoch: 016/050 | Batch 100/110 | Loss: 0.03\n",
      "Epoch: 017/050 | Batch 000/110 | Loss: 0.20\n",
      "Epoch: 017/050 | Batch 020/110 | Loss: 0.05\n",
      "Epoch: 017/050 | Batch 040/110 | Loss: 0.08\n",
      "Epoch: 017/050 | Batch 060/110 | Loss: 0.12\n",
      "Epoch: 017/050 | Batch 080/110 | Loss: 0.17\n",
      "Epoch: 017/050 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 018/050 | Batch 000/110 | Loss: 0.12\n",
      "Epoch: 018/050 | Batch 020/110 | Loss: 0.14\n",
      "Epoch: 018/050 | Batch 040/110 | Loss: 0.13\n",
      "Reached desired loss, stopping training.\n",
      "\n",
      "\n",
      "Learning rate: 0.006\n",
      "Epoch: 001/050 | Batch 000/110 | Loss: 1.30\n",
      "Epoch: 001/050 | Batch 020/110 | Loss: 0.84\n",
      "Epoch: 001/050 | Batch 040/110 | Loss: 0.67\n",
      "Epoch: 001/050 | Batch 060/110 | Loss: 0.48\n",
      "Epoch: 001/050 | Batch 080/110 | Loss: 0.47\n",
      "Epoch: 001/050 | Batch 100/110 | Loss: 0.34\n",
      "Epoch: 002/050 | Batch 000/110 | Loss: 0.38\n",
      "Epoch: 002/050 | Batch 020/110 | Loss: 0.40\n",
      "Epoch: 002/050 | Batch 040/110 | Loss: 0.35\n",
      "Epoch: 002/050 | Batch 060/110 | Loss: 0.26\n",
      "Epoch: 002/050 | Batch 080/110 | Loss: 0.19\n",
      "Epoch: 002/050 | Batch 100/110 | Loss: 0.22\n",
      "Epoch: 003/050 | Batch 000/110 | Loss: 0.23\n",
      "Epoch: 003/050 | Batch 020/110 | Loss: 0.13\n",
      "Epoch: 003/050 | Batch 040/110 | Loss: 0.16\n",
      "Epoch: 003/050 | Batch 060/110 | Loss: 0.13\n",
      "Epoch: 003/050 | Batch 080/110 | Loss: 0.20\n",
      "Epoch: 003/050 | Batch 100/110 | Loss: 0.20\n",
      "Epoch: 004/050 | Batch 000/110 | Loss: 0.12\n",
      "Epoch: 004/050 | Batch 020/110 | Loss: 0.21\n",
      "Epoch: 004/050 | Batch 040/110 | Loss: 0.13\n",
      "Epoch: 004/050 | Batch 060/110 | Loss: 0.22\n",
      "Epoch: 004/050 | Batch 080/110 | Loss: 0.14\n",
      "Epoch: 004/050 | Batch 100/110 | Loss: 0.41\n",
      "Epoch: 005/050 | Batch 000/110 | Loss: 0.27\n",
      "Epoch: 005/050 | Batch 020/110 | Loss: 0.19\n",
      "Epoch: 005/050 | Batch 040/110 | Loss: 0.18\n",
      "Epoch: 005/050 | Batch 060/110 | Loss: 0.05\n",
      "Epoch: 005/050 | Batch 080/110 | Loss: 0.11\n",
      "Epoch: 005/050 | Batch 100/110 | Loss: 0.11\n",
      "Epoch: 006/050 | Batch 000/110 | Loss: 0.14\n",
      "Epoch: 006/050 | Batch 020/110 | Loss: 0.10\n",
      "Epoch: 006/050 | Batch 040/110 | Loss: 0.26\n",
      "Epoch: 006/050 | Batch 060/110 | Loss: 0.17\n",
      "Epoch: 006/050 | Batch 080/110 | Loss: 0.12\n",
      "Epoch: 006/050 | Batch 100/110 | Loss: 0.13\n",
      "Epoch: 007/050 | Batch 000/110 | Loss: 0.07\n",
      "Epoch: 007/050 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 007/050 | Batch 040/110 | Loss: 0.11\n",
      "Epoch: 007/050 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 007/050 | Batch 080/110 | Loss: 0.12\n",
      "Epoch: 007/050 | Batch 100/110 | Loss: 0.11\n",
      "Epoch: 008/050 | Batch 000/110 | Loss: 0.12\n",
      "Epoch: 008/050 | Batch 020/110 | Loss: 0.02\n",
      "Epoch: 008/050 | Batch 040/110 | Loss: 0.02\n",
      "Epoch: 008/050 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 008/050 | Batch 080/110 | Loss: 0.08\n",
      "Epoch: 008/050 | Batch 100/110 | Loss: 0.09\n",
      "Epoch: 009/050 | Batch 000/110 | Loss: 0.07\n",
      "Epoch: 009/050 | Batch 020/110 | Loss: 0.15\n",
      "Epoch: 009/050 | Batch 040/110 | Loss: 0.11\n",
      "Epoch: 009/050 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 009/050 | Batch 080/110 | Loss: 0.05\n",
      "Epoch: 009/050 | Batch 100/110 | Loss: 0.05\n",
      "Epoch: 010/050 | Batch 000/110 | Loss: 0.11\n",
      "Epoch: 010/050 | Batch 020/110 | Loss: 0.12\n",
      "Epoch: 010/050 | Batch 040/110 | Loss: 0.27\n",
      "Epoch: 010/050 | Batch 060/110 | Loss: 0.12\n",
      "Epoch: 010/050 | Batch 080/110 | Loss: 0.17\n",
      "Epoch: 010/050 | Batch 100/110 | Loss: 0.08\n",
      "Epoch: 011/050 | Batch 000/110 | Loss: 0.06\n",
      "Epoch: 011/050 | Batch 020/110 | Loss: 0.06\n",
      "Epoch: 011/050 | Batch 040/110 | Loss: 0.10\n",
      "Epoch: 011/050 | Batch 060/110 | Loss: 0.04\n",
      "Epoch: 011/050 | Batch 080/110 | Loss: 0.14\n",
      "Epoch: 011/050 | Batch 100/110 | Loss: 0.05\n",
      "Epoch: 012/050 | Batch 000/110 | Loss: 0.19\n",
      "Epoch: 012/050 | Batch 020/110 | Loss: 0.18\n",
      "Epoch: 012/050 | Batch 040/110 | Loss: 0.05\n",
      "Epoch: 012/050 | Batch 060/110 | Loss: 0.17\n",
      "Epoch: 012/050 | Batch 080/110 | Loss: 0.18\n",
      "Epoch: 012/050 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 013/050 | Batch 000/110 | Loss: 0.04\n",
      "Epoch: 013/050 | Batch 020/110 | Loss: 0.06\n",
      "Epoch: 013/050 | Batch 040/110 | Loss: 0.23\n",
      "Epoch: 013/050 | Batch 060/110 | Loss: 0.12\n",
      "Epoch: 013/050 | Batch 080/110 | Loss: 0.20\n",
      "Epoch: 013/050 | Batch 100/110 | Loss: 0.27\n",
      "Epoch: 014/050 | Batch 000/110 | Loss: 0.24\n",
      "Epoch: 014/050 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 014/050 | Batch 040/110 | Loss: 0.06\n",
      "Epoch: 014/050 | Batch 060/110 | Loss: 0.09\n",
      "Epoch: 014/050 | Batch 080/110 | Loss: 0.04\n",
      "Epoch: 014/050 | Batch 100/110 | Loss: 0.08\n",
      "Epoch: 015/050 | Batch 000/110 | Loss: 0.14\n",
      "Epoch: 015/050 | Batch 020/110 | Loss: 0.09\n",
      "Epoch: 015/050 | Batch 040/110 | Loss: 0.10\n",
      "Epoch: 015/050 | Batch 060/110 | Loss: 0.15\n",
      "Epoch: 015/050 | Batch 080/110 | Loss: 0.08\n",
      "Epoch: 015/050 | Batch 100/110 | Loss: 0.03\n",
      "Epoch: 016/050 | Batch 000/110 | Loss: 0.13\n",
      "Epoch: 016/050 | Batch 020/110 | Loss: 0.01\n",
      "Epoch: 016/050 | Batch 040/110 | Loss: 0.17\n",
      "Epoch: 016/050 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 016/050 | Batch 080/110 | Loss: 0.19\n",
      "Epoch: 016/050 | Batch 100/110 | Loss: 0.02\n",
      "Epoch: 017/050 | Batch 000/110 | Loss: 0.19\n",
      "Epoch: 017/050 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 017/050 | Batch 040/110 | Loss: 0.07\n",
      "Epoch: 017/050 | Batch 060/110 | Loss: 0.11\n",
      "Epoch: 017/050 | Batch 080/110 | Loss: 0.16\n",
      "Epoch: 017/050 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 018/050 | Batch 000/110 | Loss: 0.11\n",
      "Epoch: 018/050 | Batch 020/110 | Loss: 0.13\n",
      "Epoch: 018/050 | Batch 040/110 | Loss: 0.12\n",
      "Reached desired loss, stopping training.\n",
      "\n",
      "\n",
      "Learning rate: 0.007\n",
      "Epoch: 001/050 | Batch 000/110 | Loss: 1.30\n",
      "Epoch: 001/050 | Batch 020/110 | Loss: 0.81\n",
      "Epoch: 001/050 | Batch 040/110 | Loss: 0.63\n",
      "Epoch: 001/050 | Batch 060/110 | Loss: 0.45\n",
      "Epoch: 001/050 | Batch 080/110 | Loss: 0.41\n",
      "Epoch: 001/050 | Batch 100/110 | Loss: 0.32\n",
      "Epoch: 002/050 | Batch 000/110 | Loss: 0.36\n",
      "Epoch: 002/050 | Batch 020/110 | Loss: 0.37\n",
      "Epoch: 002/050 | Batch 040/110 | Loss: 0.33\n",
      "Epoch: 002/050 | Batch 060/110 | Loss: 0.25\n",
      "Epoch: 002/050 | Batch 080/110 | Loss: 0.17\n",
      "Epoch: 002/050 | Batch 100/110 | Loss: 0.20\n",
      "Epoch: 003/050 | Batch 000/110 | Loss: 0.21\n",
      "Epoch: 003/050 | Batch 020/110 | Loss: 0.11\n",
      "Epoch: 003/050 | Batch 040/110 | Loss: 0.14\n",
      "Epoch: 003/050 | Batch 060/110 | Loss: 0.12\n",
      "Epoch: 003/050 | Batch 080/110 | Loss: 0.18\n",
      "Epoch: 003/050 | Batch 100/110 | Loss: 0.18\n",
      "Epoch: 004/050 | Batch 000/110 | Loss: 0.11\n",
      "Epoch: 004/050 | Batch 020/110 | Loss: 0.19\n",
      "Epoch: 004/050 | Batch 040/110 | Loss: 0.12\n",
      "Epoch: 004/050 | Batch 060/110 | Loss: 0.21\n",
      "Epoch: 004/050 | Batch 080/110 | Loss: 0.13\n",
      "Epoch: 004/050 | Batch 100/110 | Loss: 0.40\n",
      "Epoch: 005/050 | Batch 000/110 | Loss: 0.26\n",
      "Epoch: 005/050 | Batch 020/110 | Loss: 0.18\n",
      "Epoch: 005/050 | Batch 040/110 | Loss: 0.16\n",
      "Epoch: 005/050 | Batch 060/110 | Loss: 0.05\n",
      "Epoch: 005/050 | Batch 080/110 | Loss: 0.10\n",
      "Epoch: 005/050 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 006/050 | Batch 000/110 | Loss: 0.13\n",
      "Epoch: 006/050 | Batch 020/110 | Loss: 0.09\n",
      "Epoch: 006/050 | Batch 040/110 | Loss: 0.25\n",
      "Epoch: 006/050 | Batch 060/110 | Loss: 0.16\n",
      "Epoch: 006/050 | Batch 080/110 | Loss: 0.11\n",
      "Epoch: 006/050 | Batch 100/110 | Loss: 0.12\n",
      "Epoch: 007/050 | Batch 000/110 | Loss: 0.07\n",
      "Epoch: 007/050 | Batch 020/110 | Loss: 0.07\n",
      "Epoch: 007/050 | Batch 040/110 | Loss: 0.10\n",
      "Epoch: 007/050 | Batch 060/110 | Loss: 0.07\n",
      "Epoch: 007/050 | Batch 080/110 | Loss: 0.11\n",
      "Epoch: 007/050 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 008/050 | Batch 000/110 | Loss: 0.11\n",
      "Epoch: 008/050 | Batch 020/110 | Loss: 0.02\n",
      "Epoch: 008/050 | Batch 040/110 | Loss: 0.01\n",
      "Epoch: 008/050 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 008/050 | Batch 080/110 | Loss: 0.07\n",
      "Epoch: 008/050 | Batch 100/110 | Loss: 0.08\n",
      "Epoch: 009/050 | Batch 000/110 | Loss: 0.06\n",
      "Epoch: 009/050 | Batch 020/110 | Loss: 0.14\n",
      "Epoch: 009/050 | Batch 040/110 | Loss: 0.10\n",
      "Epoch: 009/050 | Batch 060/110 | Loss: 0.07\n",
      "Epoch: 009/050 | Batch 080/110 | Loss: 0.04\n",
      "Epoch: 009/050 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 010/050 | Batch 000/110 | Loss: 0.10\n",
      "Epoch: 010/050 | Batch 020/110 | Loss: 0.11\n",
      "Epoch: 010/050 | Batch 040/110 | Loss: 0.26\n",
      "Epoch: 010/050 | Batch 060/110 | Loss: 0.11\n",
      "Epoch: 010/050 | Batch 080/110 | Loss: 0.16\n",
      "Epoch: 010/050 | Batch 100/110 | Loss: 0.07\n",
      "Epoch: 011/050 | Batch 000/110 | Loss: 0.05\n",
      "Epoch: 011/050 | Batch 020/110 | Loss: 0.06\n",
      "Epoch: 011/050 | Batch 040/110 | Loss: 0.10\n",
      "Epoch: 011/050 | Batch 060/110 | Loss: 0.03\n",
      "Epoch: 011/050 | Batch 080/110 | Loss: 0.13\n",
      "Epoch: 011/050 | Batch 100/110 | Loss: 0.05\n",
      "Epoch: 012/050 | Batch 000/110 | Loss: 0.18\n",
      "Epoch: 012/050 | Batch 020/110 | Loss: 0.17\n",
      "Epoch: 012/050 | Batch 040/110 | Loss: 0.04\n",
      "Epoch: 012/050 | Batch 060/110 | Loss: 0.16\n",
      "Epoch: 012/050 | Batch 080/110 | Loss: 0.17\n",
      "Epoch: 012/050 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 013/050 | Batch 000/110 | Loss: 0.03\n",
      "Epoch: 013/050 | Batch 020/110 | Loss: 0.06\n",
      "Epoch: 013/050 | Batch 040/110 | Loss: 0.22\n",
      "Epoch: 013/050 | Batch 060/110 | Loss: 0.12\n",
      "Epoch: 013/050 | Batch 080/110 | Loss: 0.19\n",
      "Epoch: 013/050 | Batch 100/110 | Loss: 0.27\n",
      "Epoch: 014/050 | Batch 000/110 | Loss: 0.22\n",
      "Epoch: 014/050 | Batch 020/110 | Loss: 0.07\n",
      "Epoch: 014/050 | Batch 040/110 | Loss: 0.06\n",
      "Epoch: 014/050 | Batch 060/110 | Loss: 0.08\n",
      "Epoch: 014/050 | Batch 080/110 | Loss: 0.03\n",
      "Epoch: 014/050 | Batch 100/110 | Loss: 0.07\n",
      "Epoch: 015/050 | Batch 000/110 | Loss: 0.13\n",
      "Epoch: 015/050 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 015/050 | Batch 040/110 | Loss: 0.10\n",
      "Epoch: 015/050 | Batch 060/110 | Loss: 0.15\n",
      "Epoch: 015/050 | Batch 080/110 | Loss: 0.08\n",
      "Epoch: 015/050 | Batch 100/110 | Loss: 0.03\n",
      "Epoch: 016/050 | Batch 000/110 | Loss: 0.12\n",
      "Epoch: 016/050 | Batch 020/110 | Loss: 0.01\n",
      "Epoch: 016/050 | Batch 040/110 | Loss: 0.16\n",
      "Epoch: 016/050 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 016/050 | Batch 080/110 | Loss: 0.19\n",
      "Epoch: 016/050 | Batch 100/110 | Loss: 0.02\n",
      "Epoch: 017/050 | Batch 000/110 | Loss: 0.19\n",
      "Epoch: 017/050 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 017/050 | Batch 040/110 | Loss: 0.07\n",
      "Epoch: 017/050 | Batch 060/110 | Loss: 0.11\n",
      "Epoch: 017/050 | Batch 080/110 | Loss: 0.15\n",
      "Epoch: 017/050 | Batch 100/110 | Loss: 0.03\n",
      "Epoch: 018/050 | Batch 000/110 | Loss: 0.10\n",
      "Reached desired loss, stopping training.\n",
      "\n",
      "\n",
      "Learning rate: 0.008\n",
      "Epoch: 001/050 | Batch 000/110 | Loss: 1.30\n",
      "Epoch: 001/050 | Batch 020/110 | Loss: 0.79\n",
      "Epoch: 001/050 | Batch 040/110 | Loss: 0.59\n",
      "Epoch: 001/050 | Batch 060/110 | Loss: 0.42\n",
      "Epoch: 001/050 | Batch 080/110 | Loss: 0.37\n",
      "Epoch: 001/050 | Batch 100/110 | Loss: 0.30\n",
      "Epoch: 002/050 | Batch 000/110 | Loss: 0.34\n",
      "Epoch: 002/050 | Batch 020/110 | Loss: 0.35\n",
      "Epoch: 002/050 | Batch 040/110 | Loss: 0.31\n",
      "Epoch: 002/050 | Batch 060/110 | Loss: 0.23\n",
      "Epoch: 002/050 | Batch 080/110 | Loss: 0.15\n",
      "Epoch: 002/050 | Batch 100/110 | Loss: 0.18\n",
      "Epoch: 003/050 | Batch 000/110 | Loss: 0.20\n",
      "Epoch: 003/050 | Batch 020/110 | Loss: 0.10\n",
      "Epoch: 003/050 | Batch 040/110 | Loss: 0.13\n",
      "Epoch: 003/050 | Batch 060/110 | Loss: 0.11\n",
      "Epoch: 003/050 | Batch 080/110 | Loss: 0.17\n",
      "Epoch: 003/050 | Batch 100/110 | Loss: 0.17\n",
      "Epoch: 004/050 | Batch 000/110 | Loss: 0.10\n",
      "Epoch: 004/050 | Batch 020/110 | Loss: 0.18\n",
      "Epoch: 004/050 | Batch 040/110 | Loss: 0.11\n",
      "Epoch: 004/050 | Batch 060/110 | Loss: 0.21\n",
      "Epoch: 004/050 | Batch 080/110 | Loss: 0.12\n",
      "Epoch: 004/050 | Batch 100/110 | Loss: 0.39\n",
      "Epoch: 005/050 | Batch 000/110 | Loss: 0.25\n",
      "Epoch: 005/050 | Batch 020/110 | Loss: 0.17\n",
      "Epoch: 005/050 | Batch 040/110 | Loss: 0.15\n",
      "Epoch: 005/050 | Batch 060/110 | Loss: 0.04\n",
      "Epoch: 005/050 | Batch 080/110 | Loss: 0.09\n",
      "Epoch: 005/050 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 006/050 | Batch 000/110 | Loss: 0.13\n",
      "Epoch: 006/050 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 006/050 | Batch 040/110 | Loss: 0.24\n",
      "Epoch: 006/050 | Batch 060/110 | Loss: 0.15\n",
      "Epoch: 006/050 | Batch 080/110 | Loss: 0.10\n",
      "Epoch: 006/050 | Batch 100/110 | Loss: 0.11\n",
      "Epoch: 007/050 | Batch 000/110 | Loss: 0.06\n",
      "Epoch: 007/050 | Batch 020/110 | Loss: 0.06\n",
      "Epoch: 007/050 | Batch 040/110 | Loss: 0.10\n",
      "Epoch: 007/050 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 007/050 | Batch 080/110 | Loss: 0.11\n",
      "Epoch: 007/050 | Batch 100/110 | Loss: 0.09\n",
      "Epoch: 008/050 | Batch 000/110 | Loss: 0.11\n",
      "Epoch: 008/050 | Batch 020/110 | Loss: 0.02\n",
      "Epoch: 008/050 | Batch 040/110 | Loss: 0.01\n",
      "Epoch: 008/050 | Batch 060/110 | Loss: 0.05\n",
      "Epoch: 008/050 | Batch 080/110 | Loss: 0.07\n",
      "Epoch: 008/050 | Batch 100/110 | Loss: 0.08\n",
      "Epoch: 009/050 | Batch 000/110 | Loss: 0.06\n",
      "Epoch: 009/050 | Batch 020/110 | Loss: 0.13\n",
      "Epoch: 009/050 | Batch 040/110 | Loss: 0.10\n",
      "Epoch: 009/050 | Batch 060/110 | Loss: 0.07\n",
      "Epoch: 009/050 | Batch 080/110 | Loss: 0.04\n",
      "Epoch: 009/050 | Batch 100/110 | Loss: 0.04\n",
      "Epoch: 010/050 | Batch 000/110 | Loss: 0.09\n",
      "Epoch: 010/050 | Batch 020/110 | Loss: 0.10\n",
      "Epoch: 010/050 | Batch 040/110 | Loss: 0.26\n",
      "Epoch: 010/050 | Batch 060/110 | Loss: 0.10\n",
      "Epoch: 010/050 | Batch 080/110 | Loss: 0.15\n",
      "Epoch: 010/050 | Batch 100/110 | Loss: 0.07\n",
      "Epoch: 011/050 | Batch 000/110 | Loss: 0.05\n",
      "Reached desired loss, stopping training.\n",
      "\n",
      "\n",
      "Learning rate: 0.009\n",
      "Epoch: 001/050 | Batch 000/110 | Loss: 1.35\n",
      "Epoch: 001/050 | Batch 020/110 | Loss: 0.42\n",
      "Epoch: 001/050 | Batch 040/110 | Loss: 0.39\n",
      "Epoch: 001/050 | Batch 060/110 | Loss: 0.35\n",
      "Epoch: 001/050 | Batch 080/110 | Loss: 0.21\n",
      "Epoch: 001/050 | Batch 100/110 | Loss: 0.24\n",
      "Epoch: 002/050 | Batch 000/110 | Loss: 0.35\n",
      "Epoch: 002/050 | Batch 020/110 | Loss: 0.31\n",
      "Epoch: 002/050 | Batch 040/110 | Loss: 0.28\n",
      "Epoch: 002/050 | Batch 060/110 | Loss: 0.19\n",
      "Epoch: 002/050 | Batch 080/110 | Loss: 0.10\n",
      "Epoch: 002/050 | Batch 100/110 | Loss: 0.17\n",
      "Epoch: 003/050 | Batch 000/110 | Loss: 0.18\n",
      "Epoch: 003/050 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 003/050 | Batch 040/110 | Loss: 0.11\n",
      "Epoch: 003/050 | Batch 060/110 | Loss: 0.10\n",
      "Epoch: 003/050 | Batch 080/110 | Loss: 0.16\n",
      "Epoch: 003/050 | Batch 100/110 | Loss: 0.15\n",
      "Epoch: 004/050 | Batch 000/110 | Loss: 0.09\n",
      "Epoch: 004/050 | Batch 020/110 | Loss: 0.16\n",
      "Epoch: 004/050 | Batch 040/110 | Loss: 0.11\n",
      "Epoch: 004/050 | Batch 060/110 | Loss: 0.20\n",
      "Epoch: 004/050 | Batch 080/110 | Loss: 0.12\n",
      "Epoch: 004/050 | Batch 100/110 | Loss: 0.40\n",
      "Epoch: 005/050 | Batch 000/110 | Loss: 0.26\n",
      "Epoch: 005/050 | Batch 020/110 | Loss: 0.16\n",
      "Epoch: 005/050 | Batch 040/110 | Loss: 0.14\n",
      "Epoch: 005/050 | Batch 060/110 | Loss: 0.04\n",
      "Epoch: 005/050 | Batch 080/110 | Loss: 0.09\n",
      "Epoch: 005/050 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 006/050 | Batch 000/110 | Loss: 0.11\n",
      "Epoch: 006/050 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 006/050 | Batch 040/110 | Loss: 0.26\n",
      "Epoch: 006/050 | Batch 060/110 | Loss: 0.14\n",
      "Epoch: 006/050 | Batch 080/110 | Loss: 0.09\n",
      "Epoch: 006/050 | Batch 100/110 | Loss: 0.10\n",
      "Epoch: 007/050 | Batch 000/110 | Loss: 0.06\n",
      "Epoch: 007/050 | Batch 020/110 | Loss: 0.06\n",
      "Epoch: 007/050 | Batch 040/110 | Loss: 0.10\n",
      "Epoch: 007/050 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 007/050 | Batch 080/110 | Loss: 0.10\n",
      "Reached desired loss, stopping training.\n",
      "\n",
      "\n",
      "Learning rate: 0.01\n",
      "Epoch: 001/050 | Batch 000/110 | Loss: 0.78\n",
      "Epoch: 001/050 | Batch 020/110 | Loss: 0.48\n",
      "Epoch: 001/050 | Batch 040/110 | Loss: 0.36\n",
      "Epoch: 001/050 | Batch 060/110 | Loss: 0.25\n",
      "Epoch: 001/050 | Batch 080/110 | Loss: 0.26\n",
      "Epoch: 001/050 | Batch 100/110 | Loss: 0.17\n",
      "Epoch: 002/050 | Batch 000/110 | Loss: 0.42\n",
      "Epoch: 002/050 | Batch 020/110 | Loss: 0.24\n",
      "Epoch: 002/050 | Batch 040/110 | Loss: 0.27\n",
      "Epoch: 002/050 | Batch 060/110 | Loss: 0.17\n",
      "Epoch: 002/050 | Batch 080/110 | Loss: 0.08\n",
      "Epoch: 002/050 | Batch 100/110 | Loss: 0.19\n",
      "Epoch: 003/050 | Batch 000/110 | Loss: 0.13\n",
      "Epoch: 003/050 | Batch 020/110 | Loss: 0.08\n",
      "Epoch: 003/050 | Batch 040/110 | Loss: 0.09\n",
      "Epoch: 003/050 | Batch 060/110 | Loss: 0.10\n",
      "Epoch: 003/050 | Batch 080/110 | Loss: 0.12\n",
      "Epoch: 003/050 | Batch 100/110 | Loss: 0.13\n",
      "Epoch: 004/050 | Batch 000/110 | Loss: 0.07\n",
      "Epoch: 004/050 | Batch 020/110 | Loss: 0.13\n",
      "Epoch: 004/050 | Batch 040/110 | Loss: 0.09\n",
      "Epoch: 004/050 | Batch 060/110 | Loss: 0.17\n",
      "Epoch: 004/050 | Batch 080/110 | Loss: 0.10\n",
      "Epoch: 004/050 | Batch 100/110 | Loss: 0.33\n",
      "Epoch: 005/050 | Batch 000/110 | Loss: 0.20\n",
      "Epoch: 005/050 | Batch 020/110 | Loss: 0.13\n",
      "Epoch: 005/050 | Batch 040/110 | Loss: 0.14\n",
      "Epoch: 005/050 | Batch 060/110 | Loss: 0.03\n",
      "Epoch: 005/050 | Batch 080/110 | Loss: 0.07\n",
      "Epoch: 005/050 | Batch 100/110 | Loss: 0.08\n",
      "Epoch: 006/050 | Batch 000/110 | Loss: 0.10\n",
      "Epoch: 006/050 | Batch 020/110 | Loss: 0.07\n",
      "Epoch: 006/050 | Batch 040/110 | Loss: 0.20\n",
      "Epoch: 006/050 | Batch 060/110 | Loss: 0.11\n",
      "Epoch: 006/050 | Batch 080/110 | Loss: 0.08\n",
      "Epoch: 006/050 | Batch 100/110 | Loss: 0.07\n",
      "Epoch: 007/050 | Batch 000/110 | Loss: 0.05\n",
      "Epoch: 007/050 | Batch 020/110 | Loss: 0.04\n",
      "Epoch: 007/050 | Batch 040/110 | Loss: 0.08\n",
      "Epoch: 007/050 | Batch 060/110 | Loss: 0.06\n",
      "Epoch: 007/050 | Batch 080/110 | Loss: 0.09\n",
      "Reached desired loss, stopping training.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = []\n",
    "for lr in [lr/1000.0 for lr in range(1, 11)]:\n",
    "    print(f'Learning rate: {lr}')\n",
    "    model = LogisticRegression(num_features=4)\n",
    "    epoch = train_logistic_model(model, train_loader, learning_rate=lr)\n",
    "    epochs.append(epoch)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e89f54d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 35, 18, 18, 18, 18, 18, 11, 7, 7]\n",
      "[0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01]\n"
     ]
    }
   ],
   "source": [
    "lr_rates = [lr/1000.0 for lr in range(1, 11)]\n",
    "print(epochs)\n",
    "print(lr_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9760c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def plot_lr_vs_epochs(epochs, lr_rates):\n",
    "    \"\"\"\n",
    "    Plot learning rates versus epochs needed to reach convergence\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(lr_rates, epochs, 'o-', linewidth=2, markersize=8, color='C0')\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Epochs until Convergence')\n",
    "    plt.title('Effect of Learning Rate on Convergence Speed')\n",
    "    plt.grid(True)\n",
    "    \n",
    "     # Add annotations for each point\n",
    "    for i, (lr, epoch) in enumerate(zip(lr_rates, epochs)):\n",
    "         plt.annotate(f'({lr:.4f}, {epoch})', \n",
    "                      xy=(lr, epoch),\n",
    "                      xytext=(5, 5),\n",
    "                      textcoords='offset points')\n",
    "    \n",
    "     # Add annotation about loss rate threshold\n",
    "    plt.annotate('Loss rate < 0.01', \n",
    "                 xy=(0.5, 0.95),\n",
    "                 xytext=(0.5, 0.95),\n",
    "                 xycoords='axes fraction',\n",
    "                 ha='center',\n",
    "                 va='top',\n",
    "                 color='C0',\n",
    "                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Save the plot as a PNG file\n",
    "    plt.savefig('lr_vs_epochs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "662ea99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_lr_vs_epochs(epochs, lr_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41396cca-8920-4edd-9075-588c03d81f01",
   "metadata": {},
   "source": [
    "## 6) Evaluating the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f458e2cc-011c-48e5-b66a-5ef568114242",
   "metadata": {},
   "source": [
    "Again, reusing the code from Unit 3.6, we will calculate the training and validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b6473aa-98ac-4ffe-84b5-cb5a2d511018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "\n",
    "    model = model.eval()\n",
    "    \n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "    \n",
    "    for idx, (features, class_labels) in enumerate(dataloader):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probas = model(features)\n",
    "        \n",
    "        pred = torch.where(probas > 0.5, 1, 0)\n",
    "        lab = class_labels.view(pred.shape).to(pred.dtype)\n",
    "\n",
    "        compare = lab == pred\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "\n",
    "    return correct / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70de800b-4138-49ac-b4cc-e89605b78a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 97.45%\n"
     ]
    }
   ],
   "source": [
    "train_acc = compute_accuracy(model, train_loader)\n",
    "print(f\"Training Accuracy: {train_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e464b-cc38-41b7-9d7f-6baafba73f56",
   "metadata": {},
   "source": [
    "<font color='red'>Notice that the code validation accuracy is not shown? It's part of the exercise to implement it :)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edead56-db64-4667-8007-937ab1974ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b5a130b-a752-4ec4-9d3b-58fd20d0bc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 97.09%\n"
     ]
    }
   ],
   "source": [
    "val_acc = compute_accuracy(model, val_loader)\n",
    "print(f\"Validation Accuracy: {val_acc*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
